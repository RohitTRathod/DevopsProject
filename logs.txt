
==> Audit <==
|--------------|-----------------------------|----------|-----------------------|---------|---------------------|---------------------|
|   Command    |            Args             | Profile  |         User          | Version |     Start Time      |      End Time       |
|--------------|-----------------------------|----------|-----------------------|---------|---------------------|---------------------|
| config       | view                        | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 13:31 IST | 28 Dec 24 13:31 IST |
| config       | view                        | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 13:32 IST | 28 Dec 24 13:32 IST |
| stop         |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 13:33 IST | 28 Dec 24 13:34 IST |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 13:34 IST | 28 Dec 24 13:35 IST |
| service      | devops-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 14:07 IST |                     |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 14:22 IST | 28 Dec 24 14:22 IST |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 16:33 IST | 28 Dec 24 16:35 IST |
| service      | cuboid --url                | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 16:40 IST |                     |
| service      | cuboid-app-deployment --url | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 16:51 IST |                     |
| service      | spook-app-service           | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 16:55 IST |                     |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 17:04 IST | 28 Dec 24 17:04 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 17:06 IST | 28 Dec 24 17:06 IST |
| service      | test-app-service            | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 17:21 IST | 28 Dec 24 17:23 IST |
| service      | test-app-service            | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 28 Dec 24 17:23 IST |                     |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 29 Dec 24 00:24 IST | 29 Dec 24 00:26 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 16:56 IST | 02 Jan 25 16:56 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 16:56 IST | 02 Jan 25 16:56 IST |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 17:48 IST | 02 Jan 25 17:50 IST |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 19:48 IST | 02 Jan 25 19:50 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 19:49 IST | 02 Jan 25 19:49 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 19:57 IST | 02 Jan 25 19:57 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 19:57 IST | 02 Jan 25 19:57 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 19:58 IST | 02 Jan 25 19:58 IST |
| service      | ibmapp-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:06 IST |                     |
| service      | ibmapp-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:07 IST |                     |
| service      | ibmapp-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:12 IST |                     |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:12 IST | 02 Jan 25 20:12 IST |
| service      | ibmapp-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:13 IST |                     |
| service      | ibmapp-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:14 IST |                     |
| service      | ibmapp-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:18 IST | 02 Jan 25 20:19 IST |
| service      | ibmapp-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:23 IST | 02 Jan 25 20:29 IST |
| service      | ibmapp-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:29 IST | 02 Jan 25 20:32 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:31 IST | 02 Jan 25 20:31 IST |
| service      | ibmapp-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:32 IST | 02 Jan 25 20:37 IST |
| service      | ibmapp-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:37 IST |                     |
| service      | spook-app-service           | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 20:38 IST |                     |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 21:21 IST |                     |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 21:22 IST | 02 Jan 25 21:23 IST |
| service      | ibmapp-service              | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 21:26 IST | 02 Jan 25 21:27 IST |
| service      | ibmapp-app-service          | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 21:31 IST | 02 Jan 25 21:58 IST |
| stop         |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 21:59 IST | 02 Jan 25 21:59 IST |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 21:59 IST |                     |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 22:49 IST |                     |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 23:00 IST | 02 Jan 25 23:13 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Jan 25 23:07 IST | 02 Jan 25 23:07 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 03 Jan 25 08:42 IST | 03 Jan 25 08:42 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 03 Jan 25 08:48 IST | 03 Jan 25 08:48 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 03 Jan 25 08:49 IST | 03 Jan 25 08:49 IST |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Feb 25 13:40 IST | 02 Feb 25 13:40 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Feb 25 13:47 IST |                     |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Feb 25 13:47 IST | 02 Feb 25 13:47 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Feb 25 13:47 IST | 02 Feb 25 13:47 IST |
| service      | ibmapp-app-service          | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 02 Feb 25 14:55 IST | 02 Feb 25 15:35 IST |
| update-check |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 23 Feb 25 16:30 IST | 23 Feb 25 16:30 IST |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 23 Feb 25 16:31 IST |                     |
| start        |                             | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 23 Feb 25 16:32 IST | 23 Feb 25 16:33 IST |
| service      | ibmapp-app-service          | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 23 Feb 25 17:05 IST |                     |
| service      | ibmapp                      | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 23 Feb 25 17:07 IST |                     |
| service      | ibmapp-app-service          | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 23 Feb 25 17:08 IST |                     |
| service      | ibmapp-app-service          | minikube | ROHITTR-PAVILIO\admin | v1.34.0 | 23 Feb 25 17:15 IST |                     |
|--------------|-----------------------------|----------|-----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/02/23 16:32:10
Running on machine: RohitTR-Pavilion
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0223 16:32:10.181688    9628 out.go:345] Setting OutFile to fd 104 ...
I0223 16:32:10.184973    9628 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0223 16:32:10.184973    9628 out.go:358] Setting ErrFile to fd 108...
I0223 16:32:10.184973    9628 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0223 16:32:10.275341    9628 out.go:352] Setting JSON to false
I0223 16:32:10.287158    9628 start.go:129] hostinfo: {"hostname":"RohitTR-Pavilion","uptime":5146,"bootTime":1740303384,"procs":311,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.3194 Build 26100.3194","kernelVersion":"10.0.26100.3194 Build 26100.3194","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"c318948a-a825-4a9a-9970-10609cfe7409"}
W0223 16:32:10.287717    9628 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0223 16:32:10.302926    9628 out.go:177] * minikube v1.34.0 on Microsoft Windows 11 Home Single Language 10.0.26100.3194 Build 26100.3194
I0223 16:32:10.306957    9628 notify.go:220] Checking for updates...
I0223 16:32:10.310459    9628 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0223 16:32:10.313978    9628 driver.go:394] Setting default libvirt URI to qemu:///system
I0223 16:32:10.916664    9628 docker.go:123] docker version: linux-27.2.0:Docker Desktop 4.34.3 (170107)
I0223 16:32:10.992312    9628 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0223 16:32:15.130650    9628 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (4.1383375s)
I0223 16:32:15.135890    9628 info.go:266] docker info: {ID:1222c405-2e4b-4a37-8154-fa55da6e07c1 Containers:35 ContainersRunning:2 ContainersPaused:0 ContainersStopped:33 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:57 OomKillDisable:true NGoroutines:77 SystemTime:2025-02-23 11:02:15.054789858 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4026568704 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.2.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8fc6bcff51318944179630522a095cc9dbf9f353 Expected:8fc6bcff51318944179630522a095cc9dbf9f353} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.2-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.13.0]] Warnings:<nil>}}
I0223 16:32:15.145067    9628 out.go:177] * Using the docker driver based on existing profile
I0223 16:32:15.149713    9628 start.go:297] selected driver: docker
I0223 16:32:15.149713    9628 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0223 16:32:15.149713    9628 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0223 16:32:15.323336    9628 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0223 16:32:15.987135    9628 info.go:266] docker info: {ID:1222c405-2e4b-4a37-8154-fa55da6e07c1 Containers:35 ContainersRunning:2 ContainersPaused:0 ContainersStopped:33 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:57 OomKillDisable:true NGoroutines:77 SystemTime:2025-02-23 11:02:15.944288913 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4026568704 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.2.0 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8fc6bcff51318944179630522a095cc9dbf9f353 Expected:8fc6bcff51318944179630522a095cc9dbf9f353} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.2-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.2-desktop.2] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.13.0]] Warnings:<nil>}}
I0223 16:32:16.120750    9628 cni.go:84] Creating CNI manager for ""
I0223 16:32:16.120750    9628 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0223 16:32:16.121341    9628 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0223 16:32:16.125288    9628 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0223 16:32:16.132110    9628 cache.go:121] Beginning downloading kic base image for docker with docker
I0223 16:32:16.137571    9628 out.go:177] * Pulling base image v0.0.45 ...
I0223 16:32:16.147548    9628 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0223 16:32:16.147548    9628 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45 in local docker daemon
I0223 16:32:16.148546    9628 preload.go:146] Found local preload: C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I0223 16:32:16.148546    9628 cache.go:56] Caching tarball of preloaded images
I0223 16:32:16.149747    9628 preload.go:172] Found C:\Users\admin\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0223 16:32:16.149747    9628 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I0223 16:32:16.150891    9628 profile.go:143] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
W0223 16:32:16.434282    9628 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45 is of wrong architecture
I0223 16:32:16.434282    9628 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45 to local cache
I0223 16:32:16.437148    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\kicbase:v0.0.45.tar -> C:\Users\admin\.minikube\cache\kic\amd64\kicbase_v0.0.45.tar
I0223 16:32:16.437667    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\kicbase:v0.0.45.tar -> C:\Users\admin\.minikube\cache\kic\amd64\kicbase_v0.0.45.tar
I0223 16:32:16.438214    9628 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45 in local cache directory
I0223 16:32:16.440509    9628 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45 in local cache directory, skipping pull
I0223 16:32:16.440509    9628 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45 exists in cache, skipping pull
I0223 16:32:16.440509    9628 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45 as a tarball
I0223 16:32:16.440509    9628 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45 from local cache
I0223 16:32:16.441067    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\kicbase:v0.0.45.tar -> C:\Users\admin\.minikube\cache\kic\amd64\kicbase_v0.0.45.tar
I0223 16:32:16.465586    9628 cache.go:168] failed to download gcr.io/k8s-minikube/kicbase:v0.0.45, will try fallback image if available: tarball: unexpected EOF
I0223 16:32:16.465586    9628 image.go:79] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I0223 16:32:16.674008    9628 cache.go:149] Downloading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0223 16:32:16.674588    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\admin\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0223 16:32:16.675242    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\admin\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0223 16:32:16.675242    9628 image.go:63] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I0223 16:32:16.675796    9628 image.go:66] Found docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I0223 16:32:16.675796    9628 image.go:135] docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I0223 16:32:16.675796    9628 cache.go:152] successfully saved docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I0223 16:32:16.675796    9628 cache.go:162] Loading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I0223 16:32:16.676352    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\admin\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0223 16:32:16.691574    9628 cache.go:168] failed to download docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85, will try fallback image if available: tarball: unexpected EOF
I0223 16:32:16.691574    9628 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45 in local docker daemon
W0223 16:32:16.954817    9628 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45 is of wrong architecture
I0223 16:32:16.954817    9628 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45 to local cache
I0223 16:32:16.955327    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\kicbase:v0.0.45.tar -> C:\Users\admin\.minikube\cache\kic\amd64\kicbase_v0.0.45.tar
I0223 16:32:16.955886    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\kicbase:v0.0.45.tar -> C:\Users\admin\.minikube\cache\kic\amd64\kicbase_v0.0.45.tar
I0223 16:32:16.955886    9628 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45 in local cache directory
I0223 16:32:16.955886    9628 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45 in local cache directory, skipping pull
I0223 16:32:16.955886    9628 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45 exists in cache, skipping pull
I0223 16:32:16.955886    9628 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45 as a tarball
I0223 16:32:16.955886    9628 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45 from local cache
I0223 16:32:16.956414    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\kicbase:v0.0.45.tar -> C:\Users\admin\.minikube\cache\kic\amd64\kicbase_v0.0.45.tar
I0223 16:32:16.957014    9628 cache.go:168] failed to download gcr.io/k8s-minikube/kicbase:v0.0.45, will try fallback image if available: tarball: unexpected EOF
I0223 16:32:16.957014    9628 image.go:79] Checking for docker.io/kicbase/stable:v0.0.45 in local docker daemon
I0223 16:32:17.160821    9628 cache.go:149] Downloading docker.io/kicbase/stable:v0.0.45 to local cache
I0223 16:32:17.160821    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\stable:v0.0.45.tar -> C:\Users\admin\.minikube\cache\kic\amd64\stable_v0.0.45.tar
I0223 16:32:17.161371    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\stable:v0.0.45.tar -> C:\Users\admin\.minikube\cache\kic\amd64\stable_v0.0.45.tar
I0223 16:32:17.161371    9628 image.go:63] Checking for docker.io/kicbase/stable:v0.0.45 in local cache directory
I0223 16:32:17.162461    9628 image.go:66] Found docker.io/kicbase/stable:v0.0.45 in local cache directory, skipping pull
I0223 16:32:17.162461    9628 image.go:135] docker.io/kicbase/stable:v0.0.45 exists in cache, skipping pull
I0223 16:32:17.162461    9628 cache.go:152] successfully saved docker.io/kicbase/stable:v0.0.45 as a tarball
I0223 16:32:17.162461    9628 cache.go:162] Loading docker.io/kicbase/stable:v0.0.45 from local cache
I0223 16:32:17.163044    9628 localpath.go:151] windows sanitize: C:\Users\admin\.minikube\cache\kic\amd64\stable:v0.0.45.tar -> C:\Users\admin\.minikube\cache\kic\amd64\stable_v0.0.45.tar
I0223 16:32:17.186323    9628 cache.go:168] failed to download docker.io/kicbase/stable:v0.0.45, will try fallback image if available: tarball: unexpected EOF
E0223 16:32:17.186835    9628 cache.go:189] Error downloading kic artifacts:  failed to download kic base image or any fallback image
I0223 16:32:17.187401    9628 cache.go:194] Successfully downloaded all kic artifacts
I0223 16:32:17.192564    9628 start.go:360] acquireMachinesLock for minikube: {Name:mk643f7e9da83601d587a510a4be0c611c47f2bc Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0223 16:32:17.192564    9628 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0223 16:32:17.193125    9628 start.go:96] Skipping create...Using existing machine configuration
I0223 16:32:17.193735    9628 fix.go:54] fixHost starting: 
I0223 16:32:17.350187    9628 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 16:32:17.477554    9628 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0223 16:32:17.477554    9628 fix.go:138] unexpected machine state, will restart: <nil>
I0223 16:32:17.495332    9628 out.go:177] * Restarting existing docker container for "minikube" ...
I0223 16:32:17.578395    9628 cli_runner.go:164] Run: docker start minikube
I0223 16:32:20.613971    9628 cli_runner.go:217] Completed: docker start minikube: (3.035576s)
I0223 16:32:20.762538    9628 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 16:32:20.987004    9628 kic.go:430] container "minikube" state is running.
I0223 16:32:21.142417    9628 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0223 16:32:21.413770    9628 profile.go:143] Saving config to C:\Users\admin\.minikube\profiles\minikube\config.json ...
I0223 16:32:21.422142    9628 machine.go:93] provisionDockerMachine start ...
I0223 16:32:21.521685    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:21.779804    9628 main.go:141] libmachine: Using SSH client type: native
I0223 16:32:21.814603    9628 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb2c9c0] 0xb2f5a0 <nil>  [] 0s} 127.0.0.1 51595 <nil> <nil>}
I0223 16:32:21.814603    9628 main.go:141] libmachine: About to run SSH command:
hostname
I0223 16:32:21.845046    9628 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0223 16:32:25.253739    9628 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0223 16:32:25.254938    9628 ubuntu.go:169] provisioning hostname "minikube"
I0223 16:32:25.357685    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:25.593781    9628 main.go:141] libmachine: Using SSH client type: native
I0223 16:32:25.594878    9628 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb2c9c0] 0xb2f5a0 <nil>  [] 0s} 127.0.0.1 51595 <nil> <nil>}
I0223 16:32:25.594878    9628 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0223 16:32:25.907867    9628 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0223 16:32:26.054908    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:26.279490    9628 main.go:141] libmachine: Using SSH client type: native
I0223 16:32:26.279490    9628 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb2c9c0] 0xb2f5a0 <nil>  [] 0s} 127.0.0.1 51595 <nil> <nil>}
I0223 16:32:26.281014    9628 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0223 16:32:26.930201    9628 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0223 16:32:26.931356    9628 ubuntu.go:175] set auth options {CertDir:C:\Users\admin\.minikube CaCertPath:C:\Users\admin\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\admin\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\admin\.minikube\machines\server.pem ServerKeyPath:C:\Users\admin\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\admin\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\admin\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\admin\.minikube}
I0223 16:32:26.931931    9628 ubuntu.go:177] setting up certificates
I0223 16:32:26.932440    9628 provision.go:84] configureAuth start
I0223 16:32:27.045916    9628 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0223 16:32:27.217041    9628 provision.go:143] copyHostCerts
I0223 16:32:27.246664    9628 exec_runner.go:144] found C:\Users\admin\.minikube/ca.pem, removing ...
I0223 16:32:27.247241    9628 exec_runner.go:203] rm: C:\Users\admin\.minikube\ca.pem
I0223 16:32:27.249302    9628 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\ca.pem --> C:\Users\admin\.minikube/ca.pem (1074 bytes)
I0223 16:32:27.307422    9628 exec_runner.go:144] found C:\Users\admin\.minikube/cert.pem, removing ...
I0223 16:32:27.307422    9628 exec_runner.go:203] rm: C:\Users\admin\.minikube\cert.pem
I0223 16:32:27.309171    9628 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\cert.pem --> C:\Users\admin\.minikube/cert.pem (1119 bytes)
I0223 16:32:27.337690    9628 exec_runner.go:144] found C:\Users\admin\.minikube/key.pem, removing ...
I0223 16:32:27.337690    9628 exec_runner.go:203] rm: C:\Users\admin\.minikube\key.pem
I0223 16:32:27.338777    9628 exec_runner.go:151] cp: C:\Users\admin\.minikube\certs\key.pem --> C:\Users\admin\.minikube/key.pem (1679 bytes)
I0223 16:32:27.340873    9628 provision.go:117] generating server cert: C:\Users\admin\.minikube\machines\server.pem ca-key=C:\Users\admin\.minikube\certs\ca.pem private-key=C:\Users\admin\.minikube\certs\ca-key.pem org=admin.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0223 16:32:27.907391    9628 provision.go:177] copyRemoteCerts
I0223 16:32:27.943150    9628 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0223 16:32:28.033068    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:28.211434    9628 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51595 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0223 16:32:28.381652    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0223 16:32:28.462585    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0223 16:32:28.539255    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0223 16:32:28.611921    9628 provision.go:87] duration metric: took 1.6752457s to configureAuth
I0223 16:32:28.612610    9628 ubuntu.go:193] setting minikube options for container-runtime
I0223 16:32:28.613764    9628 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0223 16:32:28.706636    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:28.923648    9628 main.go:141] libmachine: Using SSH client type: native
I0223 16:32:28.924175    9628 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb2c9c0] 0xb2f5a0 <nil>  [] 0s} 127.0.0.1 51595 <nil> <nil>}
I0223 16:32:28.924175    9628 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0223 16:32:29.166287    9628 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0223 16:32:29.166287    9628 ubuntu.go:71] root file system type: overlay
I0223 16:32:29.169506    9628 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0223 16:32:29.273311    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:29.457004    9628 main.go:141] libmachine: Using SSH client type: native
I0223 16:32:29.457835    9628 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb2c9c0] 0xb2f5a0 <nil>  [] 0s} 127.0.0.1 51595 <nil> <nil>}
I0223 16:32:29.458342    9628 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0223 16:32:29.729878    9628 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0223 16:32:29.841669    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:30.038857    9628 main.go:141] libmachine: Using SSH client type: native
I0223 16:32:30.039905    9628 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb2c9c0] 0xb2f5a0 <nil>  [] 0s} 127.0.0.1 51595 <nil> <nil>}
I0223 16:32:30.039905    9628 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0223 16:32:30.317237    9628 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0223 16:32:30.317237    9628 machine.go:96] duration metric: took 8.895095s to provisionDockerMachine
I0223 16:32:30.319909    9628 start.go:293] postStartSetup for "minikube" (driver="docker")
I0223 16:32:30.319909    9628 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0223 16:32:30.343081    9628 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0223 16:32:30.484057    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:30.670166    9628 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51595 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0223 16:32:31.019631    9628 ssh_runner.go:195] Run: cat /etc/os-release
I0223 16:32:31.030403    9628 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0223 16:32:31.030942    9628 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0223 16:32:31.030942    9628 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0223 16:32:31.030942    9628 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0223 16:32:31.031805    9628 filesync.go:126] Scanning C:\Users\admin\.minikube\addons for local assets ...
I0223 16:32:31.033498    9628 filesync.go:126] Scanning C:\Users\admin\.minikube\files for local assets ...
I0223 16:32:31.034196    9628 start.go:296] duration metric: took 714.2874ms for postStartSetup
I0223 16:32:31.232057    9628 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0223 16:32:31.325084    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:31.470067    9628 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51595 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0223 16:32:31.761117    9628 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0223 16:32:31.772467    9628 fix.go:56] duration metric: took 14.5793417s for fixHost
I0223 16:32:31.772467    9628 start.go:83] releasing machines lock for "minikube", held for 14.5799031s
I0223 16:32:31.847778    9628 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0223 16:32:32.012679    9628 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0223 16:32:32.095242    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:32.127967    9628 ssh_runner.go:195] Run: cat /version.json
I0223 16:32:32.221621    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:32.233519    9628 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51595 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0223 16:32:32.352803    9628 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51595 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
W0223 16:32:32.366971    9628 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0223 16:32:32.670126    9628 ssh_runner.go:195] Run: systemctl --version
I0223 16:32:32.860143    9628 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0223 16:32:32.900762    9628 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0223 16:32:32.932537    9628 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0223 16:32:32.952217    9628 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0223 16:32:32.983900    9628 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0223 16:32:32.984409    9628 start.go:495] detecting cgroup driver to use...
I0223 16:32:32.984409    9628 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0223 16:32:32.988940    9628 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0223 16:32:33.185622    9628 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0223 16:32:33.367741    9628 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0223 16:32:33.401469    9628 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0223 16:32:33.553815    9628 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0223 16:32:33.731584    9628 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0223 16:32:33.905828    9628 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0223 16:32:34.119277    9628 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0223 16:32:34.302250    9628 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0223 16:32:34.486059    9628 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0223 16:32:34.713305    9628 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0223 16:32:34.939183    9628 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0223 16:32:34.997698    9628 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0223 16:32:35.049277    9628 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0223 16:32:35.097970    9628 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0223 16:32:35.385961    9628 ssh_runner.go:195] Run: sudo systemctl restart containerd
W0223 16:32:35.510035    9628 out.go:270] ! Failing to connect to https://registry.k8s.io/ from both inside the minikube container and host machine
W0223 16:32:35.514268    9628 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0223 16:32:35.997253    9628 start.go:495] detecting cgroup driver to use...
I0223 16:32:35.997253    9628 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0223 16:32:36.030232    9628 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0223 16:32:36.081029    9628 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0223 16:32:36.100504    9628 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0223 16:32:36.247474    9628 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0223 16:32:36.471710    9628 ssh_runner.go:195] Run: which cri-dockerd
I0223 16:32:36.518157    9628 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0223 16:32:36.556645    9628 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0223 16:32:36.667576    9628 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0223 16:32:37.155851    9628 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0223 16:32:37.482396    9628 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0223 16:32:37.482396    9628 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0223 16:32:37.551576    9628 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0223 16:32:37.875197    9628 ssh_runner.go:195] Run: sudo systemctl restart docker
I0223 16:32:39.096906    9628 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.2211964s)
I0223 16:32:39.118887    9628 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0223 16:32:39.161518    9628 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0223 16:32:39.223082    9628 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0223 16:32:39.266350    9628 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0223 16:32:39.586823    9628 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0223 16:32:39.891650    9628 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0223 16:32:40.195534    9628 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0223 16:32:40.252445    9628 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0223 16:32:40.296101    9628 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0223 16:32:40.534636    9628 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0223 16:32:41.110271    9628 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0223 16:32:41.239442    9628 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0223 16:32:41.248649    9628 start.go:563] Will wait 60s for crictl version
I0223 16:32:41.431256    9628 ssh_runner.go:195] Run: which crictl
I0223 16:32:41.482934    9628 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0223 16:32:41.850721    9628 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I0223 16:32:41.967712    9628 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0223 16:32:42.276470    9628 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0223 16:32:42.334488    9628 out.go:235] * Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I0223 16:32:42.434690    9628 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0223 16:32:42.912762    9628 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0223 16:32:43.039449    9628 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0223 16:32:43.050194    9628 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0223 16:32:43.147841    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0223 16:32:43.276130    9628 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0223 16:32:43.277368    9628 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0223 16:32:43.368273    9628 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0223 16:32:43.424903    9628 docker.go:685] Got preloaded images: -- stdout --
rohittrathod/ibmproject:latest
rohittrathod/ibmproject:<none>
rohittrathod/ibmproject:<none>
rohittrathod/pretest:latest
rohittrathod/pretest:<none>
rohittrathod/pretest:<none>
rohittrathod/cuboid:latest
rohittrathod/testing:latest
docker:latest
jenkins/inbound-agent:3283.v92c105e0f819-4
bitnami/kubectl:latest
rohittrathod/ml-app:latest
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
maven:alpine

-- /stdout --
I0223 16:32:43.424903    9628 docker.go:615] Images already preloaded, skipping extraction
I0223 16:32:43.539688    9628 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0223 16:32:43.598059    9628 docker.go:685] Got preloaded images: -- stdout --
rohittrathod/ibmproject:latest
rohittrathod/ibmproject:<none>
rohittrathod/ibmproject:<none>
rohittrathod/pretest:latest
rohittrathod/pretest:<none>
rohittrathod/pretest:<none>
rohittrathod/cuboid:latest
rohittrathod/testing:latest
docker:latest
jenkins/inbound-agent:3283.v92c105e0f819-4
bitnami/kubectl:latest
rohittrathod/ml-app:latest
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
maven:alpine

-- /stdout --
I0223 16:32:43.598611    9628 cache_images.go:84] Images are preloaded, skipping loading
I0223 16:32:43.598611    9628 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I0223 16:32:43.600395    9628 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0223 16:32:43.726190    9628 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0223 16:32:44.318400    9628 cni.go:84] Creating CNI manager for ""
I0223 16:32:44.318978    9628 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0223 16:32:44.318978    9628 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0223 16:32:44.319548    9628 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0223 16:32:44.319548    9628 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0223 16:32:44.341563    9628 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I0223 16:32:44.365816    9628 binaries.go:44] Found k8s binaries, skipping transfer
I0223 16:32:44.387209    9628 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0223 16:32:44.404691    9628 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0223 16:32:44.439417    9628 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0223 16:32:44.470479    9628 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0223 16:32:44.695468    9628 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0223 16:32:44.704947    9628 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0223 16:32:44.749442    9628 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0223 16:32:45.086176    9628 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0223 16:32:45.132972    9628 certs.go:68] Setting up C:\Users\admin\.minikube\profiles\minikube for IP: 192.168.49.2
I0223 16:32:45.132972    9628 certs.go:194] generating shared ca certs ...
I0223 16:32:45.134179    9628 certs.go:226] acquiring lock for ca certs: {Name:mkfd7a320f78a704b321a6e757eb4483941c4372 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0223 16:32:45.168362    9628 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\admin\.minikube\ca.key
I0223 16:32:45.209477    9628 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\admin\.minikube\proxy-client-ca.key
I0223 16:32:45.210246    9628 certs.go:256] generating profile certs ...
I0223 16:32:45.213180    9628 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\admin\.minikube\profiles\minikube\client.key
I0223 16:32:45.250376    9628 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\admin\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0223 16:32:45.291549    9628 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\admin\.minikube\profiles\minikube\proxy-client.key
I0223 16:32:45.300391    9628 certs.go:484] found cert: C:\Users\admin\.minikube\certs\ca-key.pem (1675 bytes)
I0223 16:32:45.300953    9628 certs.go:484] found cert: C:\Users\admin\.minikube\certs\ca.pem (1074 bytes)
I0223 16:32:45.301506    9628 certs.go:484] found cert: C:\Users\admin\.minikube\certs\cert.pem (1119 bytes)
I0223 16:32:45.302742    9628 certs.go:484] found cert: C:\Users\admin\.minikube\certs\key.pem (1679 bytes)
I0223 16:32:45.313934    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0223 16:32:45.367249    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0223 16:32:45.427928    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0223 16:32:45.494042    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0223 16:32:45.554914    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0223 16:32:45.627466    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0223 16:32:45.686266    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0223 16:32:45.755258    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0223 16:32:46.183268    9628 ssh_runner.go:362] scp C:\Users\admin\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0223 16:32:46.768162    9628 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0223 16:32:47.142387    9628 ssh_runner.go:195] Run: openssl version
I0223 16:32:47.237801    9628 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0223 16:32:47.486019    9628 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0223 16:32:47.499961    9628 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 14 06:28 /usr/share/ca-certificates/minikubeCA.pem
I0223 16:32:47.676231    9628 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0223 16:32:47.745523    9628 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0223 16:32:48.013340    9628 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0223 16:32:48.262494    9628 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0223 16:32:48.597118    9628 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0223 16:32:48.841343    9628 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0223 16:32:49.126638    9628 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0223 16:32:49.336754    9628 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0223 16:32:49.558118    9628 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0223 16:32:49.612768    9628 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\admin:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0223 16:32:49.727153    9628 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0223 16:32:50.973480    9628 ssh_runner.go:235] Completed: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}: (1.2463267s)
I0223 16:32:51.015903    9628 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0223 16:32:51.381748    9628 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0223 16:32:51.385235    9628 kubeadm.go:593] restartPrimaryControlPlane start ...
I0223 16:32:51.422766    9628 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0223 16:32:51.556249    9628 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0223 16:32:51.688561    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0223 16:32:51.877003    9628 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:51042"
I0223 16:32:51.877519    9628 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:51042, want: 127.0.0.1:51593
I0223 16:32:51.878739    9628 kubeconfig.go:62] C:\Users\admin\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I0223 16:32:51.881038    9628 lock.go:35] WriteFile acquiring C:\Users\admin\.kube\config: {Name:mk0f6f650be7ab5b3e1ae83f14563cf1eb0bd30b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0223 16:32:51.990176    9628 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0223 16:32:52.177760    9628 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0223 16:32:52.177760    9628 kubeadm.go:597] duration metric: took 792.5253ms to restartPrimaryControlPlane
I0223 16:32:52.177760    9628 kubeadm.go:394] duration metric: took 2.5713056s to StartCluster
I0223 16:32:52.177760    9628 settings.go:142] acquiring lock: {Name:mk5c419c3fabff3b09a504acb297c33a24a93937 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0223 16:32:52.178844    9628 settings.go:150] Updating kubeconfig:  C:\Users\admin\.kube\config
I0223 16:32:52.183249    9628 lock.go:35] WriteFile acquiring C:\Users\admin\.kube\config: {Name:mk0f6f650be7ab5b3e1ae83f14563cf1eb0bd30b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0223 16:32:52.187273    9628 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0223 16:32:52.189150    9628 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0223 16:32:52.194703    9628 out.go:177] * Verifying Kubernetes components...
I0223 16:32:52.198045    9628 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0223 16:32:52.198045    9628 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0223 16:32:52.198406    9628 addons.go:69] Setting dashboard=true in profile "minikube"
I0223 16:32:52.198406    9628 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0223 16:32:52.203660    9628 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0223 16:32:52.203986    9628 addons.go:243] addon storage-provisioner should already be in state true
I0223 16:32:52.203986    9628 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0223 16:32:52.203986    9628 addons.go:234] Setting addon dashboard=true in "minikube"
W0223 16:32:52.203986    9628 addons.go:243] addon dashboard should already be in state true
I0223 16:32:52.209066    9628 host.go:66] Checking if "minikube" exists ...
I0223 16:32:52.209760    9628 host.go:66] Checking if "minikube" exists ...
I0223 16:32:52.225931    9628 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0223 16:32:52.418804    9628 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 16:32:52.419911    9628 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 16:32:52.421032    9628 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 16:32:52.645235    9628 out.go:177]   - Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0223 16:32:52.646806    9628 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0223 16:32:52.646806    9628 addons.go:243] addon default-storageclass should already be in state true
I0223 16:32:52.647802    9628 host.go:66] Checking if "minikube" exists ...
I0223 16:32:52.656547    9628 out.go:177]   - Using image docker.io/kubernetesui/dashboard:v2.7.0
I0223 16:32:52.663786    9628 addons.go:431] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0223 16:32:52.664378    9628 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0223 16:32:52.669720    9628 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0223 16:32:52.672542    9628 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0223 16:32:52.672542    9628 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0223 16:32:52.742185    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:52.753409    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:52.902541    9628 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 16:32:52.926254    9628 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51595 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0223 16:32:52.938859    9628 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51595 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0223 16:32:52.982946    9628 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0223 16:32:53.028529    9628 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I0223 16:32:53.029092    9628 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0223 16:32:53.134048    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 16:32:53.208743    9628 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0223 16:32:53.320184    9628 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51595 SSHKeyPath:C:\Users\admin\.minikube\machines\minikube\id_rsa Username:docker}
I0223 16:32:53.327514    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0223 16:32:53.363208    9628 api_server.go:52] waiting for apiserver process to appear ...
I0223 16:32:53.370178    9628 addons.go:431] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0223 16:32:53.370178    9628 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0223 16:32:53.390832    9628 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0223 16:32:53.880439    9628 addons.go:431] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0223 16:32:53.880439    9628 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0223 16:32:54.264832    9628 addons.go:431] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0223 16:32:54.264832    9628 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0223 16:32:54.523059    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0223 16:32:54.763220    9628 addons.go:431] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0223 16:32:54.763220    9628 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0223 16:32:55.355146    9628 addons.go:431] installing /etc/kubernetes/addons/dashboard-role.yaml
I0223 16:32:55.355659    9628 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0223 16:32:55.760469    9628 addons.go:431] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0223 16:32:55.760469    9628 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0223 16:32:56.177513    9628 addons.go:431] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0223 16:32:56.177513    9628 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0223 16:32:56.655822    9628 addons.go:431] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0223 16:32:56.655822    9628 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0223 16:32:57.252302    9628 addons.go:431] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0223 16:32:57.252302    9628 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0223 16:32:57.798129    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0223 16:32:58.175354    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (4.8472357s)
W0223 16:32:58.175354    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:32:58.175354    9628 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (4.7845221s)
I0223 16:32:58.175354    9628 retry.go:31] will retry after 316.447801ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:32:58.204862    9628 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0223 16:32:58.250659    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.7276003s)
W0223 16:32:58.250659    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:32:58.250659    9628 retry.go:31] will retry after 155.790369ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:32:58.437889    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0223 16:32:58.461625    9628 api_server.go:72] duration metric: took 6.2724757s to wait for apiserver process to appear ...
I0223 16:32:58.461625    9628 api_server.go:88] waiting for apiserver healthz status ...
W0223 16:32:58.461625    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:32:58.461625    9628 retry.go:31] will retry after 357.822709ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:32:58.471585    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:32:58.502311    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:32:58.531762    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0223 16:32:58.781781    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:32:58.781781    9628 retry.go:31] will retry after 420.832604ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0223 16:32:58.804490    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:32:58.804490    9628 retry.go:31] will retry after 203.966453ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:32:58.864678    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0223 16:32:58.973778    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:32:58.981341    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:32:59.061044    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0223 16:32:59.233098    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0223 16:32:59.468994    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:32:59.474093    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:32:59.967819    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:32:59.978421    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:33:00.264116    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (1.3994373s)
W0223 16:33:00.264116    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:00.264116    9628 retry.go:31] will retry after 424.145604ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:00.467776    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:00.481865    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:33:00.719691    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0223 16:33:00.856657    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.7956131s)
W0223 16:33:00.856657    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:00.856657    9628 retry.go:31] will retry after 324.039735ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:00.968480    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:00.968991    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.7358929s)
W0223 16:33:00.968991    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:00.968991    9628 retry.go:31] will retry after 356.048417ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:00.986661    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:33:01.212787    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0223 16:33:01.362834    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0223 16:33:01.471385    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:01.480930    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:33:01.770142    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (1.0504513s)
W0223 16:33:01.770872    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:01.770872    9628 retry.go:31] will retry after 373.14592ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:01.967260    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:01.978691    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:33:02.194137    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0223 16:33:02.475538    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:02.480726    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:33:02.974869    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:02.982577    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:33:03.161396    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.9486084s)
W0223 16:33:03.161396    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:03.161396    9628 retry.go:31] will retry after 965.468603ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:03.162041    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.7992065s)
W0223 16:33:03.162744    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:03.162744    9628 retry.go:31] will retry after 586.847991ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:03.475893    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:03.493496    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:33:03.797094    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0223 16:33:03.980608    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:04.021165    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": EOF
I0223 16:33:04.211292    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0223 16:33:04.269317    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (2.0751805s)
W0223 16:33:04.269317    9628 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:04.269317    9628 retry.go:31] will retry after 563.114336ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0223 16:33:04.467862    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:04.892241    9628 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0223 16:33:09.472551    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0223 16:33:09.472551    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:14.479048    9628 api_server.go:269] stopped: https://127.0.0.1:51593/healthz: Get "https://127.0.0.1:51593/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0223 16:33:14.479048    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:15.545604    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0223 16:33:15.545604    9628 api_server.go:103] status: https://127.0.0.1:51593/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0223 16:33:15.545604    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:15.573516    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0223 16:33:15.573516    9628 api_server.go:103] status: https://127.0.0.1:51593/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0223 16:33:15.975010    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:16.148632    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0223 16:33:16.148632    9628 api_server.go:103] status: https://127.0.0.1:51593/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0223 16:33:16.465718    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:16.650838    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0223 16:33:16.650838    9628 api_server.go:103] status: https://127.0.0.1:51593/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0223 16:33:16.961810    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:17.064417    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0223 16:33:17.065067    9628 api_server.go:103] status: https://127.0.0.1:51593/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0223 16:33:17.466972    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:17.467670    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (13.6698776s)
I0223 16:33:17.664573    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0223 16:33:17.664573    9628 api_server.go:103] status: https://127.0.0.1:51593/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0223 16:33:17.963094    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:18.065339    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0223 16:33:18.065339    9628 api_server.go:103] status: https://127.0.0.1:51593/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0223 16:33:18.462995    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:18.477448    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0223 16:33:18.477448    9628 api_server.go:103] status: https://127.0.0.1:51593/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0223 16:33:18.972230    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:19.063883    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0223 16:33:19.063883    9628 api_server.go:103] status: https://127.0.0.1:51593/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0223 16:33:19.463220    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:19.478272    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0223 16:33:19.479080    9628 api_server.go:103] status: https://127.0.0.1:51593/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0223 16:33:19.974373    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:20.147205    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0223 16:33:20.147205    9628 api_server.go:103] status: https://127.0.0.1:51593/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0223 16:33:20.469380    9628 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51593/healthz ...
I0223 16:33:20.640312    9628 api_server.go:279] https://127.0.0.1:51593/healthz returned 200:
ok
I0223 16:33:20.670636    9628 api_server.go:141] control plane version: v1.31.0
I0223 16:33:20.670636    9628 api_server.go:131] duration metric: took 22.209011s to wait for apiserver health ...
I0223 16:33:20.671872    9628 system_pods.go:43] waiting for kube-system pods to appear ...
I0223 16:33:20.973167    9628 system_pods.go:59] 7 kube-system pods found
I0223 16:33:20.973167    9628 system_pods.go:61] "coredns-6f6b679f8f-w4vd8" [f4a30fee-383f-4259-99b5-41a198d7c9b3] Running
I0223 16:33:20.973167    9628 system_pods.go:61] "etcd-minikube" [f70efe36-ac5e-42f8-8f61-aed07f6094d2] Running
I0223 16:33:20.973167    9628 system_pods.go:61] "kube-apiserver-minikube" [ebc48f4f-d68e-4312-8268-772eed0576f5] Running
I0223 16:33:20.973167    9628 system_pods.go:61] "kube-controller-manager-minikube" [e484c4a6-a23c-4767-8079-0d4c682ac5a8] Running
I0223 16:33:20.973167    9628 system_pods.go:61] "kube-proxy-vm5lj" [de01344f-dd57-4847-85f7-ca6e4f09b99d] Running
I0223 16:33:20.973167    9628 system_pods.go:61] "kube-scheduler-minikube" [ea761ff6-cc20-4a2c-9c54-44f9339d4286] Running
I0223 16:33:20.973167    9628 system_pods.go:61] "storage-provisioner" [5788b811-11f3-42f2-a67b-9c73df9fc1a4] Running
I0223 16:33:20.973167    9628 system_pods.go:74] duration metric: took 301.2946ms to wait for pod list to return data ...
I0223 16:33:20.973167    9628 kubeadm.go:582] duration metric: took 28.7840173s to wait for: map[apiserver:true system_pods:true]
I0223 16:33:20.973167    9628 node_conditions.go:102] verifying NodePressure condition ...
I0223 16:33:21.067989    9628 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0223 16:33:21.068530    9628 node_conditions.go:123] node cpu capacity is 8
I0223 16:33:21.068530    9628 node_conditions.go:105] duration metric: took 95.3628ms to run NodePressure ...
I0223 16:33:21.068530    9628 start.go:241] waiting for startup goroutines ...
I0223 16:33:23.252563    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (19.0412712s)
I0223 16:33:36.557397    9628 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (31.6651564s)
I0223 16:33:36.565696    9628 out.go:177] * Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0223 16:33:36.572879    9628 out.go:177] * Enabled addons: default-storageclass, storage-provisioner, dashboard
I0223 16:33:36.581401    9628 addons.go:510] duration metric: took 44.3936833s for enable addons: enabled=[default-storageclass storage-provisioner dashboard]
I0223 16:33:36.581401    9628 start.go:246] waiting for cluster config update ...
I0223 16:33:36.581401    9628 start.go:255] writing updated cluster config ...
I0223 16:33:36.874733    9628 ssh_runner.go:195] Run: rm -f paused
I0223 16:33:37.962186    9628 start.go:600] kubectl: 1.30.2, cluster: 1.31.0 (minor skew: 1)
I0223 16:33:37.967228    9628 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 23 11:02:39 minikube dockerd[1123]: time="2025-02-23T11:02:39.005307442Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Feb 23 11:02:39 minikube dockerd[1123]: time="2025-02-23T11:02:39.005326049Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Feb 23 11:02:39 minikube dockerd[1123]: time="2025-02-23T11:02:39.005335753Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 23 11:02:39 minikube dockerd[1123]: time="2025-02-23T11:02:39.005368765Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Feb 23 11:02:39 minikube dockerd[1123]: time="2025-02-23T11:02:39.005506018Z" level=info msg="Daemon has completed initialization"
Feb 23 11:02:39 minikube dockerd[1123]: time="2025-02-23T11:02:39.094927208Z" level=info msg="API listen on /var/run/docker.sock"
Feb 23 11:02:39 minikube dockerd[1123]: time="2025-02-23T11:02:39.095027247Z" level=info msg="API listen on [::]:2376"
Feb 23 11:02:39 minikube systemd[1]: Started Docker Application Container Engine.
Feb 23 11:02:40 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Feb 23 11:02:40 minikube cri-dockerd[1426]: time="2025-02-23T11:02:40Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Feb 23 11:02:40 minikube cri-dockerd[1426]: time="2025-02-23T11:02:40Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Feb 23 11:02:40 minikube cri-dockerd[1426]: time="2025-02-23T11:02:40Z" level=info msg="Start docker client with request timeout 0s"
Feb 23 11:02:40 minikube cri-dockerd[1426]: time="2025-02-23T11:02:40Z" level=info msg="Hairpin mode is set to hairpin-veth"
Feb 23 11:02:41 minikube cri-dockerd[1426]: time="2025-02-23T11:02:41Z" level=info msg="Loaded network plugin cni"
Feb 23 11:02:41 minikube cri-dockerd[1426]: time="2025-02-23T11:02:41Z" level=info msg="Docker cri networking managed by network plugin cni"
Feb 23 11:02:41 minikube cri-dockerd[1426]: time="2025-02-23T11:02:41Z" level=info msg="Setting cgroupDriver cgroupfs"
Feb 23 11:02:41 minikube cri-dockerd[1426]: time="2025-02-23T11:02:41Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 23 11:02:41 minikube cri-dockerd[1426]: time="2025-02-23T11:02:41Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 23 11:02:41 minikube cri-dockerd[1426]: time="2025-02-23T11:02:41Z" level=info msg="Start cri-dockerd grpc backend"
Feb 23 11:02:41 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 23 11:02:47 minikube cri-dockerd[1426]: time="2025-02-23T11:02:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-gl2q8_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ea55a063d38602411660fd4e0da29b01b5c054e5072734bca3afdd3bdbf546e7\""
Feb 23 11:02:47 minikube cri-dockerd[1426]: time="2025-02-23T11:02:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-gl2q8_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"60cd8257b757377bcf67668010da29606092c35ec7c6d43b58a495e8e70ea804\""
Feb 23 11:02:47 minikube cri-dockerd[1426]: time="2025-02-23T11:02:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-w4vd8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5259f994c8781a1446bfbc326e36129ac212d0fda8ba7e68351b553bd4941327\""
Feb 23 11:02:47 minikube cri-dockerd[1426]: time="2025-02-23T11:02:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-w4vd8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"374450e24241e0b02be67205d0f7fea11005cf7a43276177b6a945f5ccf56111\""
Feb 23 11:02:47 minikube cri-dockerd[1426]: time="2025-02-23T11:02:47Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-695b96c756-kfhfd_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0ff6c9c6407897fc569ebbbd817d2c27dc69fc520c07824740211fefc2c58a4f\""
Feb 23 11:02:48 minikube cri-dockerd[1426]: time="2025-02-23T11:02:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-695b96c756-kfhfd_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0ff6c9c6407897fc569ebbbd817d2c27dc69fc520c07824740211fefc2c58a4f\""
Feb 23 11:02:49 minikube cri-dockerd[1426]: time="2025-02-23T11:02:49Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-w4vd8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5259f994c8781a1446bfbc326e36129ac212d0fda8ba7e68351b553bd4941327\""
Feb 23 11:02:49 minikube cri-dockerd[1426]: time="2025-02-23T11:02:49Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-gl2q8_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ea55a063d38602411660fd4e0da29b01b5c054e5072734bca3afdd3bdbf546e7\""
Feb 23 11:02:49 minikube cri-dockerd[1426]: time="2025-02-23T11:02:49Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-gl2q8_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"60cd8257b757377bcf67668010da29606092c35ec7c6d43b58a495e8e70ea804\""
Feb 23 11:02:52 minikube cri-dockerd[1426]: time="2025-02-23T11:02:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/55630375716fb0708a67055b31e13a386d1e07f2a5949932897717de04e5a7a6/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 23 11:02:52 minikube cri-dockerd[1426]: time="2025-02-23T11:02:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/23ffde0f69d3cc099debc647de795753ed9e64e5ca257d43f9887df14dcbbf20/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 23 11:02:52 minikube cri-dockerd[1426]: time="2025-02-23T11:02:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6431534688a8543dff1f05c1d74d5f3cedc134a84eb0b2479bd131e5406281c3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 23 11:02:52 minikube cri-dockerd[1426]: time="2025-02-23T11:02:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c52be3b36ba9d1af95dd01c89f97d83cb5e1814e578f3b1265aa93704ddbae43/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 23 11:02:53 minikube cri-dockerd[1426]: time="2025-02-23T11:02:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-gl2q8_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ea55a063d38602411660fd4e0da29b01b5c054e5072734bca3afdd3bdbf546e7\""
Feb 23 11:02:57 minikube cri-dockerd[1426]: time="2025-02-23T11:02:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-w4vd8_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5259f994c8781a1446bfbc326e36129ac212d0fda8ba7e68351b553bd4941327\""
Feb 23 11:03:16 minikube cri-dockerd[1426]: time="2025-02-23T11:03:16Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 23 11:03:33 minikube cri-dockerd[1426]: time="2025-02-23T11:03:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d026b57af9f203ae70fb468d76b40cec9df2e2c7cf31ab29e5769cf8ac77f3c3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 23 11:03:33 minikube cri-dockerd[1426]: time="2025-02-23T11:03:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/63815ae107f4e0468296d03422cbe35fde781333a1e39ac7ba0cee7ff15d93ac/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 23 11:03:35 minikube cri-dockerd[1426]: time="2025-02-23T11:03:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ce6c32d56118cb066c8037fabf368bcffbead1d2cd83898555677b5620edb018/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 23 11:03:35 minikube cri-dockerd[1426]: time="2025-02-23T11:03:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/30fc474c531b9ea6fc2396558005bb3b2191ba3e473ebc17d4d32c381ae0392e/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 11:03:35 minikube cri-dockerd[1426]: time="2025-02-23T11:03:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a4ea93bc601d943901fbaf5787b515b30f651ddf22ab3cd6ab55184dc4585332/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 11:03:58 minikube dockerd[1123]: time="2025-02-23T11:03:58.393880244Z" level=info msg="ignoring event" container=e7db91fd283201ec5947c0d8803b877609dc673dddb6f5194b35cb94579db071 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 11:04:01 minikube dockerd[1123]: time="2025-02-23T11:04:01.871044425Z" level=info msg="ignoring event" container=261dfd11105873f2cecf94431b63e19e1ca2a1c9c09b06db99a1584aeb7e4cd8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 11:29:57 minikube cri-dockerd[1426]: time="2025-02-23T11:29:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/196e48c04a495990006c46a24e3b85e2e149953540d4ab6f7d2bead8228d79ea/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 11:30:05 minikube dockerd[1123]: time="2025-02-23T11:30:05.099474182Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 23 11:30:05 minikube dockerd[1123]: time="2025-02-23T11:30:05.101609051Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 23 11:30:22 minikube dockerd[1123]: time="2025-02-23T11:30:22.175225055Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 23 11:30:22 minikube dockerd[1123]: time="2025-02-23T11:30:22.175449390Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 23 11:30:55 minikube dockerd[1123]: time="2025-02-23T11:30:55.453341739Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 23 11:30:55 minikube dockerd[1123]: time="2025-02-23T11:30:55.453589273Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 23 11:31:52 minikube dockerd[1123]: time="2025-02-23T11:31:52.154154040Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 23 11:31:52 minikube dockerd[1123]: time="2025-02-23T11:31:52.154475287Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 23 11:33:17 minikube dockerd[1123]: time="2025-02-23T11:33:17.082102345Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://registry-1.docker.io/v2/rohittrathod/ibmproject/manifests/latest\": EOF"
Feb 23 11:33:17 minikube dockerd[1123]: time="2025-02-23T11:33:17.100074319Z" level=error msg="Handler for POST /v1.43/images/create returned error: Head \"https://registry-1.docker.io/v2/rohittrathod/ibmproject/manifests/latest\": EOF"
Feb 23 11:36:14 minikube dockerd[1123]: time="2025-02-23T11:36:14.271698632Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 23 11:36:14 minikube dockerd[1123]: time="2025-02-23T11:36:14.271865453Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 23 11:41:23 minikube dockerd[1123]: time="2025-02-23T11:41:23.197897634Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 23 11:41:23 minikube dockerd[1123]: time="2025-02-23T11:41:23.198205778Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Feb 23 11:46:29 minikube dockerd[1123]: time="2025-02-23T11:46:29.989951630Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Feb 23 11:46:29 minikube dockerd[1123]: time="2025-02-23T11:46:29.990266892Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
99ba2164972a5       07655ddf2eebe       43 minutes ago      Running             kubernetes-dashboard        25                  30fc474c531b9       kubernetes-dashboard-695b96c756-kfhfd
ca761964fa840       6e38f40d628db       43 minutes ago      Running             storage-provisioner         28                  63815ae107f4e       storage-provisioner
ffccd8b84ad51       115053965e86b       43 minutes ago      Running             dashboard-metrics-scraper   14                  a4ea93bc601d9       dashboard-metrics-scraper-c5db448b4-gl2q8
e4e945933311c       cbb01a7bd410d       43 minutes ago      Running             coredns                     14                  ce6c32d56118c       coredns-6f6b679f8f-w4vd8
261dfd1110587       07655ddf2eebe       43 minutes ago      Exited              kubernetes-dashboard        24                  30fc474c531b9       kubernetes-dashboard-695b96c756-kfhfd
332ca9e61afeb       ad83b2ca7b09e       43 minutes ago      Running             kube-proxy                  14                  d026b57af9f20       kube-proxy-vm5lj
e7db91fd28320       6e38f40d628db       43 minutes ago      Exited              storage-provisioner         27                  63815ae107f4e       storage-provisioner
1fe5d0577dda1       604f5db92eaa8       44 minutes ago      Running             kube-apiserver              14                  55630375716fb       kube-apiserver-minikube
35aaf06125db0       045733566833c       44 minutes ago      Running             kube-controller-manager     23                  c52be3b36ba9d       kube-controller-manager-minikube
cb910300419c8       1766f54c897f0       44 minutes ago      Running             kube-scheduler              14                  6431534688a85       kube-scheduler-minikube
9ef9a202a2a46       2e96e5913fc06       44 minutes ago      Running             etcd                        14                  23ffde0f69d3c       etcd-minikube
ffcfc19de7db4       115053965e86b       3 weeks ago         Exited              dashboard-metrics-scraper   13                  ea55a063d3860       dashboard-metrics-scraper-c5db448b4-gl2q8
4ddf34d335ef5       cbb01a7bd410d       3 weeks ago         Exited              coredns                     13                  5259f994c8781       coredns-6f6b679f8f-w4vd8
0b3bb83025550       ad83b2ca7b09e       3 weeks ago         Exited              kube-proxy                  13                  5ccc761827b1e       kube-proxy-vm5lj
1cb65518eecd4       604f5db92eaa8       3 weeks ago         Exited              kube-apiserver              13                  efd9122bbea5c       kube-apiserver-minikube
f9caadae13136       2e96e5913fc06       3 weeks ago         Exited              etcd                        13                  b1bc3f4e3f673       etcd-minikube
2c394814175ea       045733566833c       3 weeks ago         Exited              kube-controller-manager     22                  219dd39f67091       kube-controller-manager-minikube
b6976d8afe113       1766f54c897f0       3 weeks ago         Exited              kube-scheduler              13                  4b110db42aa5a       kube-scheduler-minikube


==> coredns [4ddf34d335ef] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:34883 - 14144 "HINFO IN 1142741653593180598.7010705658001725742. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.326666992s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[515508067]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (02-Feb-2025 08:10:57.934) (total time: 21076ms):
Trace[515508067]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21075ms (08:11:19.005)
Trace[515508067]: [21.076278161s] [21.076278161s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1895051895]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (02-Feb-2025 08:10:57.934) (total time: 21076ms):
Trace[1895051895]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21076ms (08:11:19.005)
Trace[1895051895]: [21.076366474s] [21.076366474s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[814253527]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (02-Feb-2025 08:10:57.934) (total time: 21076ms):
Trace[814253527]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21075ms (08:11:19.006)
Trace[814253527]: [21.076397681s] [21.076397681s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] 10.244.0.132:55591 - 1895 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.003251865s
[INFO] 10.244.0.132:55591 - 43898 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.003880954s
[INFO] 10.244.0.132:46301 - 36756 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000412259s
[INFO] 10.244.0.132:46301 - 42371 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000511773s
[INFO] 10.244.0.132:48323 - 6730 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000292542s
[INFO] 10.244.0.132:48323 - 34874 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000340948s
[INFO] 10.244.0.132:32885 - 23834 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 444 0.056287143s
[INFO] 10.244.0.132:32885 - 14633 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,rd,ra 82 0.100198518s
[INFO] 10.244.0.126:41149 - 54769 "AAAA IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000632275s
[INFO] 10.244.0.126:41149 - 58859 "A IN registry.npmjs.org.default.svc.cluster.local. udp 62 false 512" NXDOMAIN qr,aa,rd 155 0.000650577s
[INFO] 10.244.0.126:57468 - 3850 "AAAA IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.00008631s
[INFO] 10.244.0.126:57468 - 35588 "A IN registry.npmjs.org.svc.cluster.local. udp 54 false 512" NXDOMAIN qr,aa,rd 147 0.000182521s
[INFO] 10.244.0.126:53042 - 54905 "AAAA IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000035804s
[INFO] 10.244.0.126:53042 - 118 "A IN registry.npmjs.org.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000096511s
[INFO] 10.244.0.126:49819 - 13036 "AAAA IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 82 0.000109913s
[INFO] 10.244.0.126:49819 - 32488 "A IN registry.npmjs.org. udp 36 false 512" NOERROR qr,aa,rd,ra 444 0.000273732s


==> coredns [e4e945933311] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:41682 - 1907 "HINFO IN 4948098311687063296.4675792715748593703. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.6061137s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1796289196]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Feb-2025 11:03:41.791) (total time: 21052ms):
Trace[1796289196]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21050ms (11:04:02.842)
Trace[1796289196]: [21.052300144s] [21.052300144s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1007220519]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Feb-2025 11:03:41.791) (total time: 21052ms):
Trace[1007220519]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21050ms (11:04:02.842)
Trace[1007220519]: [21.052495773s] [21.052495773s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[745285285]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (23-Feb-2025 11:03:41.791) (total time: 21075ms):
Trace[745285285]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21075ms (11:04:02.866)
Trace[745285285]: [21.075488569s] [21.075488569s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_11_14T11_58_44_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 14 Nov 2024 06:28:37 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 23 Feb 2025 11:47:19 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 23 Feb 2025 11:44:07 +0000   Thu, 14 Nov 2024 06:28:29 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 23 Feb 2025 11:44:07 +0000   Thu, 14 Nov 2024 06:28:29 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 23 Feb 2025 11:44:07 +0000   Thu, 14 Nov 2024 06:28:29 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 23 Feb 2025 11:44:07 +0000   Sat, 28 Dec 2024 07:02:55 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3932196Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3932196Ki
  pods:               110
System Info:
  Machine ID:                 308bebd478d544c88a23bc8aed2abae6
  System UUID:                308bebd478d544c88a23bc8aed2abae6
  Boot ID:                    049a3402-dba6-4787-8422-7bd6e2854cdf
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     ibmapp-67dfbf9bdf-gkhfw                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
  kube-system                 coredns-6f6b679f8f-w4vd8                     100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     101d
  kube-system                 etcd-minikube                                100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         101d
  kube-system                 kube-apiserver-minikube                      250m (3%)     0 (0%)      0 (0%)           0 (0%)         101d
  kube-system                 kube-controller-manager-minikube             200m (2%)     0 (0%)      0 (0%)           0 (0%)         101d
  kube-system                 kube-proxy-vm5lj                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         101d
  kube-system                 kube-scheduler-minikube                      100m (1%)     0 (0%)      0 (0%)           0 (0%)         101d
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         101d
  kubernetes-dashboard        dashboard-metrics-scraper-c5db448b4-gl2q8    0 (0%)        0 (0%)      0 (0%)           0 (0%)         101d
  kubernetes-dashboard        kubernetes-dashboard-695b96c756-kfhfd        0 (0%)        0 (0%)      0 (0%)           0 (0%)         101d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           43m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  44m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           44m                kubelet          Starting kubelet.
  Warning  CgroupV1                           44m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced            44m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            44m (x7 over 44m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              44m (x7 over 44m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               44m (x7 over 44m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     43m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.102825] FS-Cache: Duplicate cookie detected
[  +0.002710] FS-Cache: O-cookie c=0000000c [p=00000006 fl=226 nc=0 na=1]
[  +0.001371] FS-Cache: O-cookie d=0000000071cfcf24{9p.inode} n=00000000ed152b95
[  +0.001224] FS-Cache: O-key=[8] 'dfdf090000001c00'
[  +0.001599] FS-Cache: N-cookie c=0000000d [p=00000006 fl=2 nc=0 na=1]
[  +0.003275] FS-Cache: N-cookie d=0000000071cfcf24{9p.inode} n=00000000ab59972f
[  +0.001315] FS-Cache: N-key=[8] 'dfdf090000001c00'
[  +0.145957] FS-Cache: Duplicate cookie detected
[  +0.003229] FS-Cache: O-cookie c=0000000f [p=00000006 fl=226 nc=0 na=1]
[  +0.001147] FS-Cache: O-cookie d=0000000071cfcf24{9p.inode} n=0000000027feef60
[  +0.001235] FS-Cache: O-key=[8] 'ecdf090000001800'
[  +0.000808] FS-Cache: N-cookie c=00000010 [p=00000006 fl=2 nc=0 na=1]
[  +0.001173] FS-Cache: N-cookie d=0000000071cfcf24{9p.inode} n=000000009583cd56
[  +0.001199] FS-Cache: N-key=[8] 'ecdf090000001800'
[  +0.075531] FS-Cache: Duplicate cookie detected
[  +0.002265] FS-Cache: O-cookie c=00000012 [p=00000006 fl=226 nc=0 na=1]
[  +0.001160] FS-Cache: O-cookie d=0000000071cfcf24{9p.inode} n=00000000568a1053
[  +0.001246] FS-Cache: O-key=[8] 'f8df090000002900'
[  +0.000730] FS-Cache: N-cookie c=00000013 [p=00000006 fl=2 nc=0 na=1]
[  +0.000989] FS-Cache: N-cookie d=0000000071cfcf24{9p.inode} n=00000000a2981ab4
[  +0.001306] FS-Cache: N-key=[8] 'f8df090000002900'
[  +0.672410] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000010]  failed 2
[  +0.025479] FS-Cache: Duplicate cookie detected
[  +0.000872] FS-Cache: O-cookie c=0000001b [p=00000002 fl=222 nc=0 na=1]
[  +0.000897] FS-Cache: O-cookie d=00000000f05d8aa4{9P.session} n=000000006b501dce
[  +0.001168] FS-Cache: O-key=[10] '34323934393338343432'
[  +0.000892] FS-Cache: N-cookie c=0000001c [p=00000002 fl=2 nc=0 na=1]
[  +0.001273] FS-Cache: N-cookie d=00000000f05d8aa4{9P.session} n=00000000e2590bc0
[  +0.001122] FS-Cache: N-key=[10] '34323934393338343432'
[  +0.016650] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.537549] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.028147] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.012591] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002117] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002098] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.021414] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.006011] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001479] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001491] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001805] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Feb23 11:01] netlink: 'init': attribute type 4 has an invalid length.
[ +14.188216] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +3.991113] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001885] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.023765] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.012435] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003445] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.004170] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003330] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002216] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.006667] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002692] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.004623] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002288] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.013034] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.004418] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +6.365473] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Feb23 11:02] tmpfs: Unknown parameter 'noswap'
[Feb23 11:19] hrtimer: interrupt took 16420415 ns


==> etcd [9ef9a202a2a4] <==
{"level":"info","ts":"2025-02-23T11:03:36.026873Z","caller":"traceutil/trace.go:171","msg":"trace[2085111422] transaction","detail":"{read_only:false; response_revision:53397; number_of_response:1; }","duration":"193.115527ms","start":"2025-02-23T11:03:35.833724Z","end":"2025-02-23T11:03:36.026839Z","steps":["trace[2085111422] 'process raft request'  (duration: 132.6198ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:03:36.429446Z","caller":"traceutil/trace.go:171","msg":"trace[928024589] transaction","detail":"{read_only:false; response_revision:53399; number_of_response:1; }","duration":"172.018181ms","start":"2025-02-23T11:03:36.257323Z","end":"2025-02-23T11:03:36.429342Z","steps":["trace[928024589] 'process raft request'  (duration: 171.114972ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:03:37.066877Z","caller":"traceutil/trace.go:171","msg":"trace[852966557] transaction","detail":"{read_only:false; response_revision:53403; number_of_response:1; }","duration":"126.539383ms","start":"2025-02-23T11:03:36.940303Z","end":"2025-02-23T11:03:37.066842Z","steps":["trace[852966557] 'process raft request'  (duration: 96.909839ms)","trace[852966557] 'compare'  (duration: 29.115453ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-23T11:03:40.733446Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"180.137313ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035471519713087 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:53390 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128035471519713085 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2025-02-23T11:03:40.733636Z","caller":"traceutil/trace.go:171","msg":"trace[402122140] transaction","detail":"{read_only:false; response_revision:53413; number_of_response:1; }","duration":"183.97422ms","start":"2025-02-23T11:03:40.549634Z","end":"2025-02-23T11:03:40.733609Z","steps":["trace[402122140] 'compare'  (duration: 174.103255ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:03:41.259816Z","caller":"traceutil/trace.go:171","msg":"trace[1779503222] transaction","detail":"{read_only:false; response_revision:53418; number_of_response:1; }","duration":"106.699291ms","start":"2025-02-23T11:03:41.153056Z","end":"2025-02-23T11:03:41.259756Z","steps":["trace[1779503222] 'process raft request'  (duration: 106.528771ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:05:15.452346Z","caller":"traceutil/trace.go:171","msg":"trace[1045632915] transaction","detail":"{read_only:false; response_revision:53514; number_of_response:1; }","duration":"139.84678ms","start":"2025-02-23T11:05:15.312474Z","end":"2025-02-23T11:05:15.452321Z","steps":["trace[1045632915] 'process raft request'  (duration: 139.682137ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:11:24.535957Z","caller":"traceutil/trace.go:171","msg":"trace[205869725] transaction","detail":"{read_only:false; response_revision:53807; number_of_response:1; }","duration":"110.554464ms","start":"2025-02-23T11:11:24.425370Z","end":"2025-02-23T11:11:24.535925Z","steps":["trace[205869725] 'process raft request'  (duration: 110.391739ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:13:12.481692Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":53654}
{"level":"info","ts":"2025-02-23T11:13:12.568571Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":53654,"took":"80.152944ms","hash":4157240621,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1196032,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-02-23T11:13:12.569642Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4157240621,"revision":53654,"compact-revision":52964}
{"level":"info","ts":"2025-02-23T11:14:21.275023Z","caller":"traceutil/trace.go:171","msg":"trace[585457850] transaction","detail":"{read_only:false; response_revision:53948; number_of_response:1; }","duration":"109.472668ms","start":"2025-02-23T11:14:21.155613Z","end":"2025-02-23T11:14:21.265085Z","steps":["trace[585457850] 'process raft request'  (duration: 108.485721ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:14:37.587056Z","caller":"traceutil/trace.go:171","msg":"trace[490412799] transaction","detail":"{read_only:false; response_revision:53961; number_of_response:1; }","duration":"135.71061ms","start":"2025-02-23T11:14:37.451302Z","end":"2025-02-23T11:14:37.587013Z","steps":["trace[490412799] 'process raft request'  (duration: 124.133772ms)","trace[490412799] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1091; } (duration: 11.258495ms)"],"step_count":2}
{"level":"info","ts":"2025-02-23T11:15:20.501018Z","caller":"traceutil/trace.go:171","msg":"trace[449301676] transaction","detail":"{read_only:false; response_revision:53994; number_of_response:1; }","duration":"179.958046ms","start":"2025-02-23T11:15:20.321014Z","end":"2025-02-23T11:15:20.500973Z","steps":["trace[449301676] 'process raft request'  (duration: 179.620398ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:15:39.299941Z","caller":"traceutil/trace.go:171","msg":"trace[1707080681] transaction","detail":"{read_only:false; response_revision:54009; number_of_response:1; }","duration":"322.705076ms","start":"2025-02-23T11:15:38.977205Z","end":"2025-02-23T11:15:39.299910Z","steps":["trace[1707080681] 'process raft request'  (duration: 322.407743ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-23T11:15:39.367873Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-23T11:15:38.977160Z","time spent":"344.394747ms","remote":"127.0.0.1:40744","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:54007 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-02-23T11:16:08.132342Z","caller":"traceutil/trace.go:171","msg":"trace[461050438] transaction","detail":"{read_only:false; response_revision:54031; number_of_response:1; }","duration":"107.678845ms","start":"2025-02-23T11:16:08.024630Z","end":"2025-02-23T11:16:08.132309Z","steps":["trace[461050438] 'process raft request'  (duration: 107.489519ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-23T11:16:15.784254Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.534247ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-02-23T11:16:15.785441Z","caller":"traceutil/trace.go:171","msg":"trace[1574657713] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:54037; }","duration":"108.81823ms","start":"2025-02-23T11:16:15.675889Z","end":"2025-02-23T11:16:15.784707Z","steps":["trace[1574657713] 'agreement among raft nodes before linearized reading'  (duration: 107.179696ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:16:15.785318Z","caller":"traceutil/trace.go:171","msg":"trace[1766160362] linearizableReadLoop","detail":"{readStateIndex:66413; appliedIndex:66412; }","duration":"106.71553ms","start":"2025-02-23T11:16:15.675985Z","end":"2025-02-23T11:16:15.782701Z","steps":["trace[1766160362] 'read index received'  (duration: 21.118814ms)","trace[1766160362] 'applied index is now lower than readState.Index'  (duration: 85.593915ms)"],"step_count":2}
{"level":"info","ts":"2025-02-23T11:16:18.824661Z","caller":"traceutil/trace.go:171","msg":"trace[299960360] linearizableReadLoop","detail":"{readStateIndex:66415; appliedIndex:66414; }","duration":"219.205584ms","start":"2025-02-23T11:16:18.605406Z","end":"2025-02-23T11:16:18.824612Z","steps":["trace[299960360] 'read index received'  (duration: 218.75902ms)","trace[299960360] 'applied index is now lower than readState.Index'  (duration: 444.563µs)"],"step_count":2}
{"level":"info","ts":"2025-02-23T11:16:18.824968Z","caller":"traceutil/trace.go:171","msg":"trace[255165758] transaction","detail":"{read_only:false; response_revision:54039; number_of_response:1; }","duration":"393.358637ms","start":"2025-02-23T11:16:18.431582Z","end":"2025-02-23T11:16:18.824940Z","steps":["trace[255165758] 'process raft request'  (duration: 392.692742ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-23T11:16:18.825322Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-23T11:16:18.431553Z","time spent":"393.485856ms","remote":"127.0.0.1:40744","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:54038 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-02-23T11:16:18.825482Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.929386ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-02-23T11:16:18.825624Z","caller":"traceutil/trace.go:171","msg":"trace[1527993435] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:54039; }","duration":"124.085909ms","start":"2025-02-23T11:16:18.701501Z","end":"2025-02-23T11:16:18.825587Z","steps":["trace[1527993435] 'agreement among raft nodes before linearized reading'  (duration: 123.801268ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-23T11:16:18.896957Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"290.340235ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-02-23T11:16:18.898097Z","caller":"traceutil/trace.go:171","msg":"trace[1400916312] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:54039; }","duration":"292.648765ms","start":"2025-02-23T11:16:18.605391Z","end":"2025-02-23T11:16:18.898040Z","steps":["trace[1400916312] 'agreement among raft nodes before linearized reading'  (duration: 220.135016ms)","trace[1400916312] 'range keys from in-memory index tree'  (duration: 70.176315ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-23T11:16:26.310665Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"511.261553ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035471519716954 > lease_revoke:<id:70cc953279b5160d>","response":"size:30"}
{"level":"info","ts":"2025-02-23T11:16:26.310853Z","caller":"traceutil/trace.go:171","msg":"trace[457134374] linearizableReadLoop","detail":"{readStateIndex:66423; appliedIndex:66422; }","duration":"601.875324ms","start":"2025-02-23T11:16:25.708952Z","end":"2025-02-23T11:16:26.310827Z","steps":["trace[457134374] 'read index received'  (duration: 63.309µs)","trace[457134374] 'applied index is now lower than readState.Index'  (duration: 601.808114ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-23T11:16:26.310949Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"602.045848ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-02-23T11:16:26.311076Z","caller":"traceutil/trace.go:171","msg":"trace[147636892] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:54045; }","duration":"602.184868ms","start":"2025-02-23T11:16:25.708876Z","end":"2025-02-23T11:16:26.311060Z","steps":["trace[147636892] 'agreement among raft nodes before linearized reading'  (duration: 602.015644ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-23T11:16:26.313101Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-23T11:16:25.708797Z","time spent":"604.285764ms","remote":"127.0.0.1:40558","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-02-23T11:18:12.493987Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":53893}
{"level":"info","ts":"2025-02-23T11:18:12.659690Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":53893,"took":"158.891398ms","hash":1406364650,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1810432,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-23T11:18:12.660025Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1406364650,"revision":53893,"compact-revision":53654}
{"level":"info","ts":"2025-02-23T11:23:12.469884Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54130}
{"level":"info","ts":"2025-02-23T11:23:12.543872Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":54130,"took":"72.413548ms","hash":3049375411,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1687552,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-23T11:23:12.544447Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3049375411,"revision":54130,"compact-revision":53893}
{"level":"info","ts":"2025-02-23T11:28:12.458868Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54368}
{"level":"info","ts":"2025-02-23T11:28:12.494413Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":54368,"took":"34.229283ms","hash":3775983664,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1691648,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-23T11:28:12.494983Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3775983664,"revision":54368,"compact-revision":54130}
{"level":"warn","ts":"2025-02-23T11:29:30.704703Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.660727ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2025-02-23T11:29:30.704648Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.37486ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:423"}
{"level":"info","ts":"2025-02-23T11:29:30.721735Z","caller":"traceutil/trace.go:171","msg":"trace[369779586] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:54669; }","duration":"161.160016ms","start":"2025-02-23T11:29:30.549912Z","end":"2025-02-23T11:29:30.711072Z","steps":["trace[369779586] 'range keys from in-memory index tree'  (duration: 111.646971ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:29:30.722232Z","caller":"traceutil/trace.go:171","msg":"trace[1248664883] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:54669; }","duration":"161.922575ms","start":"2025-02-23T11:29:30.549627Z","end":"2025-02-23T11:29:30.711549Z","steps":["trace[1248664883] 'range keys from in-memory index tree'  (duration: 111.584566ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:30:36.625211Z","caller":"traceutil/trace.go:171","msg":"trace[1089835145] transaction","detail":"{read_only:false; response_revision:54751; number_of_response:1; }","duration":"113.338044ms","start":"2025-02-23T11:30:36.506543Z","end":"2025-02-23T11:30:36.619881Z","steps":["trace[1089835145] 'process raft request'  (duration: 112.90319ms)"],"step_count":1}
{"level":"info","ts":"2025-02-23T11:33:12.432926Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54607}
{"level":"info","ts":"2025-02-23T11:33:12.451489Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":54607,"took":"17.981576ms","hash":3683460311,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1900544,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-02-23T11:33:12.451633Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3683460311,"revision":54607,"compact-revision":54368}
{"level":"warn","ts":"2025-02-23T11:33:50.884609Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"377.428201ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2025-02-23T11:33:50.885406Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-23T11:33:50.467679Z","time spent":"416.727305ms","remote":"127.0.0.1:40600","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2025-02-23T11:33:50.890460Z","caller":"traceutil/trace.go:171","msg":"trace[20810597] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:54927; }","duration":"377.566418ms","start":"2025-02-23T11:33:50.507147Z","end":"2025-02-23T11:33:50.884714Z","steps":["trace[20810597] 'agreement among raft nodes before linearized reading'  (duration: 377.394897ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-23T11:33:50.891259Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-23T11:33:50.507059Z","time spent":"384.166942ms","remote":"127.0.0.1:40558","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-02-23T11:33:50.885913Z","caller":"traceutil/trace.go:171","msg":"trace[696318020] linearizableReadLoop","detail":"{readStateIndex:67522; appliedIndex:67521; }","duration":"377.156467ms","start":"2025-02-23T11:33:50.507156Z","end":"2025-02-23T11:33:50.884312Z","steps":["trace[696318020] 'read index received'  (duration: 376.802523ms)","trace[696318020] 'applied index is now lower than readState.Index'  (duration: 352.444µs)"],"step_count":2}
{"level":"info","ts":"2025-02-23T11:38:12.417264Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":54896}
{"level":"info","ts":"2025-02-23T11:38:12.439325Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":54896,"took":"19.478316ms","hash":2085529136,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1945600,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-02-23T11:38:12.439562Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2085529136,"revision":54896,"compact-revision":54607}
{"level":"info","ts":"2025-02-23T11:43:12.387666Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":55140}
{"level":"info","ts":"2025-02-23T11:43:12.397350Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":55140,"took":"9.155266ms","hash":158898469,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1785856,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-02-23T11:43:12.397446Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":158898469,"revision":55140,"compact-revision":54896}


==> etcd [f9caadae1313] <==
{"level":"warn","ts":"2025-02-02T10:09:18.044650Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"193.693001ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/cuboid-774b475b8c-k5zwk\" ","response":"range_response_count:1 size:2921"}
{"level":"info","ts":"2025-02-02T10:09:18.044756Z","caller":"traceutil/trace.go:171","msg":"trace[914268519] range","detail":"{range_begin:/registry/pods/default/cuboid-774b475b8c-k5zwk; range_end:; response_count:1; response_revision:51668; }","duration":"193.816418ms","start":"2025-02-02T10:09:17.850915Z","end":"2025-02-02T10:09:18.044731Z","steps":["trace[914268519] 'range keys from in-memory index tree'  (duration: 193.470971ms)"],"step_count":1}
{"level":"info","ts":"2025-02-02T10:09:19.442736Z","caller":"traceutil/trace.go:171","msg":"trace[23126478] linearizableReadLoop","detail":"{readStateIndex:63464; appliedIndex:63463; }","duration":"102.947125ms","start":"2025-02-02T10:09:19.339641Z","end":"2025-02-02T10:09:19.442589Z","steps":["trace[23126478] 'read index received'  (duration: 96.683678ms)","trace[23126478] 'applied index is now lower than readState.Index'  (duration: 6.257147ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-02T10:09:19.443464Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.79544ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-02-02T10:09:19.443600Z","caller":"traceutil/trace.go:171","msg":"trace[1564462537] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:51670; }","duration":"103.946961ms","start":"2025-02-02T10:09:19.339619Z","end":"2025-02-02T10:09:19.443566Z","steps":["trace[1564462537] 'agreement among raft nodes before linearized reading'  (duration: 103.27397ms)"],"step_count":1}
{"level":"info","ts":"2025-02-02T10:09:19.444118Z","caller":"traceutil/trace.go:171","msg":"trace[528582723] transaction","detail":"{read_only:false; response_revision:51670; number_of_response:1; }","duration":"105.897824ms","start":"2025-02-02T10:09:19.338183Z","end":"2025-02-02T10:09:19.444081Z","steps":["trace[528582723] 'process raft request'  (duration: 98.053063ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-02T10:09:19.748601Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.199254ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-02-02T10:09:19.748687Z","caller":"traceutil/trace.go:171","msg":"trace[1769554354] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:51671; }","duration":"100.297967ms","start":"2025-02-02T10:09:19.648372Z","end":"2025-02-02T10:09:19.748670Z","steps":["trace[1769554354] 'range keys from in-memory index tree'  (duration: 100.181151ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-02T10:09:19.749005Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.308239ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035004384743513 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:51665 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2025-02-02T10:09:19.749078Z","caller":"traceutil/trace.go:171","msg":"trace[845662454] linearizableReadLoop","detail":"{readStateIndex:63466; appliedIndex:63465; }","duration":"113.47735ms","start":"2025-02-02T10:09:19.635581Z","end":"2025-02-02T10:09:19.749058Z","steps":["trace[845662454] 'read index received'  (duration: 10.619736ms)","trace[845662454] 'applied index is now lower than readState.Index'  (duration: 102.856414ms)"],"step_count":2}
{"level":"info","ts":"2025-02-02T10:09:19.749400Z","caller":"traceutil/trace.go:171","msg":"trace[870943143] transaction","detail":"{read_only:false; response_revision:51672; number_of_response:1; }","duration":"198.659573ms","start":"2025-02-02T10:09:19.550720Z","end":"2025-02-02T10:09:19.749379Z","steps":["trace[870943143] 'process raft request'  (duration: 95.884971ms)","trace[870943143] 'compare'  (duration: 102.16552ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-02T10:09:19.749656Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"114.06893ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:136"}
{"level":"info","ts":"2025-02-02T10:09:19.749690Z","caller":"traceutil/trace.go:171","msg":"trace[736140800] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:51672; }","duration":"114.107035ms","start":"2025-02-02T10:09:19.635572Z","end":"2025-02-02T10:09:19.749681Z","steps":["trace[736140800] 'agreement among raft nodes before linearized reading'  (duration: 113.99552ms)"],"step_count":1}
{"level":"info","ts":"2025-02-02T10:09:23.554783Z","caller":"traceutil/trace.go:171","msg":"trace[1634865880] transaction","detail":"{read_only:false; response_revision:51686; number_of_response:1; }","duration":"114.692182ms","start":"2025-02-02T10:09:23.440019Z","end":"2025-02-02T10:09:23.554711Z","steps":["trace[1634865880] 'process raft request'  (duration: 96.06347ms)","trace[1634865880] 'compare'  (duration: 17.518956ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-02T10:09:23.851643Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.837882ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035004384743559 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/default/cuboid-774b475b8c-k5zwk\" mod_revision:51686 > success:<request_put:<key:\"/registry/pods/default/cuboid-774b475b8c-k5zwk\" value_size:2939 >> failure:<request_range:<key:\"/registry/pods/default/cuboid-774b475b8c-k5zwk\" > >>","response":"size:18"}
{"level":"info","ts":"2025-02-02T10:09:23.851801Z","caller":"traceutil/trace.go:171","msg":"trace[1726002677] transaction","detail":"{read_only:false; response_revision:51687; number_of_response:1; }","duration":"180.289981ms","start":"2025-02-02T10:09:23.671480Z","end":"2025-02-02T10:09:23.851770Z","steps":["trace[1726002677] 'process raft request'  (duration: 68.213965ms)","trace[1726002677] 'compare'  (duration: 111.691562ms)"],"step_count":2}
{"level":"info","ts":"2025-02-02T10:09:24.036880Z","caller":"traceutil/trace.go:171","msg":"trace[587997381] transaction","detail":"{read_only:false; number_of_response:1; response_revision:51688; }","duration":"178.008761ms","start":"2025-02-02T10:09:23.858842Z","end":"2025-02-02T10:09:24.036851Z","steps":["trace[587997381] 'process raft request'  (duration: 177.850539ms)"],"step_count":1}
{"level":"info","ts":"2025-02-02T10:09:24.051808Z","caller":"traceutil/trace.go:171","msg":"trace[1211344659] transaction","detail":"{read_only:false; response_revision:51689; number_of_response:1; }","duration":"105.275362ms","start":"2025-02-02T10:09:23.946497Z","end":"2025-02-02T10:09:24.051772Z","steps":["trace[1211344659] 'process raft request'  (duration: 105.116439ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-02T10:09:24.261829Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.45117ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-02-02T10:09:24.262073Z","caller":"traceutil/trace.go:171","msg":"trace[1980814644] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:51689; }","duration":"122.687504ms","start":"2025-02-02T10:09:24.139338Z","end":"2025-02-02T10:09:24.262025Z","steps":["trace[1980814644] 'range keys from in-memory index tree'  (duration: 122.340955ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-02T10:09:24.262593Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.24406ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/devops-deployment-59b78d8d49-75l2h\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-02-02T10:09:24.262700Z","caller":"traceutil/trace.go:171","msg":"trace[1720134793] range","detail":"{range_begin:/registry/pods/default/devops-deployment-59b78d8d49-75l2h; range_end:; response_count:0; response_revision:51689; }","duration":"115.351575ms","start":"2025-02-02T10:09:24.147315Z","end":"2025-02-02T10:09:24.262667Z","steps":["trace[1720134793] 'range keys from in-memory index tree'  (duration: 115.133845ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-02T10:09:24.263555Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.782693ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035004384743565 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:51674 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2025-02-02T10:09:24.263711Z","caller":"traceutil/trace.go:171","msg":"trace[1798484727] transaction","detail":"{read_only:false; response_revision:51690; number_of_response:1; }","duration":"109.959519ms","start":"2025-02-02T10:09:24.153719Z","end":"2025-02-02T10:09:24.263678Z","steps":["trace[1798484727] 'compare'  (duration: 104.570263ms)"],"step_count":1}
{"level":"info","ts":"2025-02-02T10:09:37.342830Z","caller":"traceutil/trace.go:171","msg":"trace[423365490] transaction","detail":"{read_only:false; response_revision:51707; number_of_response:1; }","duration":"104.302815ms","start":"2025-02-02T10:09:37.238481Z","end":"2025-02-02T10:09:37.342784Z","steps":["trace[423365490] 'process raft request'  (duration: 103.019917ms)"],"step_count":1}
{"level":"info","ts":"2025-02-02T10:09:40.047873Z","caller":"traceutil/trace.go:171","msg":"trace[1360885641] transaction","detail":"{read_only:false; number_of_response:1; response_revision:51715; }","duration":"114.645913ms","start":"2025-02-02T10:09:39.933203Z","end":"2025-02-02T10:09:40.047848Z","steps":["trace[1360885641] 'process raft request'  (duration: 48.41498ms)","trace[1360885641] 'compare'  (duration: 66.137719ms)"],"step_count":2}
{"level":"info","ts":"2025-02-02T10:10:36.447642Z","caller":"traceutil/trace.go:171","msg":"trace[1879828108] transaction","detail":"{read_only:false; response_revision:51763; number_of_response:1; }","duration":"106.29468ms","start":"2025-02-02T10:10:36.341286Z","end":"2025-02-02T10:10:36.447580Z","steps":["trace[1879828108] 'process raft request'  (duration: 67.504877ms)","trace[1879828108] 'compare'  (duration: 38.431352ms)"],"step_count":2}
{"level":"info","ts":"2025-02-02T10:10:46.498515Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51368}
{"level":"info","ts":"2025-02-02T10:10:46.551429Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":51368,"took":"45.408753ms","hash":3179430821,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":3338240,"current-db-size-in-use":"3.3 MB"}
{"level":"info","ts":"2025-02-02T10:10:46.551798Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3179430821,"revision":51368,"compact-revision":51105}
{"level":"warn","ts":"2025-02-02T10:13:59.472670Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"252.772339ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:423"}
{"level":"info","ts":"2025-02-02T10:13:59.482774Z","caller":"traceutil/trace.go:171","msg":"trace[1555528521] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:51924; }","duration":"310.761382ms","start":"2025-02-02T10:13:59.167863Z","end":"2025-02-02T10:13:59.478624Z","steps":["trace[1555528521] 'range keys from in-memory index tree'  (duration: 226.648834ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-02T10:13:59.483830Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-02T10:13:59.167806Z","time spent":"315.64303ms","remote":"127.0.0.1:41608","response type":"/etcdserverpb.KV/Range","request count":0,"request size":49,"response count":1,"response size":447,"request content":"key:\"/registry/services/endpoints/default/kubernetes\" "}
{"level":"info","ts":"2025-02-02T10:15:46.482345Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":51770}
{"level":"info","ts":"2025-02-02T10:15:46.594767Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":51770,"took":"105.83843ms","hash":3607313584,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":3289088,"current-db-size-in-use":"3.3 MB"}
{"level":"info","ts":"2025-02-02T10:15:46.595098Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3607313584,"revision":51770,"compact-revision":51368}
{"level":"info","ts":"2025-02-02T10:20:46.459121Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52009}
{"level":"info","ts":"2025-02-02T10:20:46.486649Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":52009,"took":"25.810518ms","hash":3905481619,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1740800,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-02T10:20:46.486843Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3905481619,"revision":52009,"compact-revision":51770}
{"level":"info","ts":"2025-02-02T10:25:46.440885Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52247}
{"level":"info","ts":"2025-02-02T10:25:46.451579Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":52247,"took":"9.614048ms","hash":4190227848,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1740800,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-02T10:25:46.451677Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4190227848,"revision":52247,"compact-revision":52009}
{"level":"info","ts":"2025-02-02T10:28:55.134094Z","caller":"traceutil/trace.go:171","msg":"trace[2003039102] linearizableReadLoop","detail":"{readStateIndex:64674; appliedIndex:64673; }","duration":"116.227399ms","start":"2025-02-02T10:28:54.936590Z","end":"2025-02-02T10:28:55.052818Z","steps":["trace[2003039102] 'read index received'  (duration: 17.685799ms)","trace[2003039102] 'applied index is now lower than readState.Index'  (duration: 98.532398ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-02T10:28:55.145504Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"199.911246ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2025-02-02T10:28:55.146147Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.125983ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-02-02T10:28:55.145997Z","caller":"traceutil/trace.go:171","msg":"trace[1626773764] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:52635; }","duration":"209.235122ms","start":"2025-02-02T10:28:54.936585Z","end":"2025-02-02T10:28:55.145820Z","steps":["trace[1626773764] 'agreement among raft nodes before linearized reading'  (duration: 117.537806ms)"],"step_count":1}
{"level":"info","ts":"2025-02-02T10:28:55.146233Z","caller":"traceutil/trace.go:171","msg":"trace[1788260252] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:52635; }","duration":"108.487074ms","start":"2025-02-02T10:28:55.037718Z","end":"2025-02-02T10:28:55.146205Z","steps":["trace[1788260252] 'agreement among raft nodes before linearized reading'  (duration: 104.03607ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-02T10:28:55.144967Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.2289ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1113"}
{"level":"info","ts":"2025-02-02T10:28:55.149740Z","caller":"traceutil/trace.go:171","msg":"trace[1397555494] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:52635; }","duration":"112.036236ms","start":"2025-02-02T10:28:55.037671Z","end":"2025-02-02T10:28:55.149707Z","steps":["trace[1397555494] 'agreement among raft nodes before linearized reading'  (duration: 104.068874ms)"],"step_count":1}
{"level":"info","ts":"2025-02-02T10:29:13.641719Z","caller":"traceutil/trace.go:171","msg":"trace[604516398] transaction","detail":"{read_only:false; response_revision:52652; number_of_response:1; }","duration":"101.696086ms","start":"2025-02-02T10:29:13.527250Z","end":"2025-02-02T10:29:13.628946Z","steps":["trace[604516398] 'process raft request'  (duration: 100.983672ms)"],"step_count":1}
{"level":"info","ts":"2025-02-02T10:29:58.381232Z","caller":"traceutil/trace.go:171","msg":"trace[1454606630] transaction","detail":"{read_only:false; response_revision:52686; number_of_response:1; }","duration":"123.392986ms","start":"2025-02-02T10:29:58.248858Z","end":"2025-02-02T10:29:58.372251Z","steps":["trace[1454606630] 'process raft request'  (duration: 123.170351ms)"],"step_count":1}
{"level":"info","ts":"2025-02-02T10:30:46.438874Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52486}
{"level":"info","ts":"2025-02-02T10:30:46.469933Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":52486,"took":"27.732641ms","hash":487762538,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1748992,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-02T10:30:46.470087Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":487762538,"revision":52486,"compact-revision":52247}
{"level":"info","ts":"2025-02-02T10:35:46.419417Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52725}
{"level":"info","ts":"2025-02-02T10:35:46.437511Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":52725,"took":"16.523007ms","hash":2088570931,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1732608,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-02-02T10:35:46.437622Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2088570931,"revision":52725,"compact-revision":52486}
{"level":"info","ts":"2025-02-02T10:40:46.394219Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":52964}
{"level":"info","ts":"2025-02-02T10:40:46.414028Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":52964,"took":"12.830628ms","hash":2172951419,"current-db-size-bytes":5627904,"current-db-size":"5.6 MB","current-db-size-in-use-bytes":1601536,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-02-02T10:40:46.414142Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2172951419,"revision":52964,"compact-revision":52725}


==> kernel <==
 11:47:22 up 46 min,  0 users,  load average: 0.56, 0.53, 0.80
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [1cb65518eecd] <==
W0202 08:10:46.817229       1 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0202 08:10:46.829130       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0202 08:10:46.829161       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0202 08:10:47.369687       1 secure_serving.go:213] Serving securely on [::]:8443
I0202 08:10:47.370031       1 controller.go:119] Starting legacy_token_tracking_controller
I0202 08:10:47.370074       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0202 08:10:47.370108       1 controller.go:142] Starting OpenAPI controller
I0202 08:10:47.370134       1 controller.go:90] Starting OpenAPI V3 controller
I0202 08:10:47.370151       1 naming_controller.go:294] Starting NamingConditionController
I0202 08:10:47.370169       1 establishing_controller.go:81] Starting EstablishingController
I0202 08:10:47.370233       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0202 08:10:47.370256       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0202 08:10:47.370270       1 crd_finalizer.go:269] Starting CRDFinalizer
I0202 08:10:47.377750       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0202 08:10:47.377801       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0202 08:10:47.377877       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0202 08:10:47.377945       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0202 08:10:47.377985       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0202 08:10:47.378115       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0202 08:10:47.378187       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I0202 08:10:47.378203       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0202 08:10:47.378242       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0202 08:10:47.377750       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0202 08:10:47.378387       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0202 08:10:47.377948       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0202 08:10:47.378761       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0202 08:10:47.413577       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0202 08:10:47.413617       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0202 08:10:47.370056       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0202 08:10:47.414425       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0202 08:10:47.414505       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0202 08:10:47.414547       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0202 08:10:47.414685       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0202 08:10:47.414777       1 controller.go:78] Starting OpenAPI AggregationController
I0202 08:10:47.414810       1 local_available_controller.go:156] Starting LocalAvailability controller
I0202 08:10:47.414816       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0202 08:10:47.378243       1 aggregator.go:169] waiting for initial CRD sync...
I0202 08:10:47.415740       1 aggregator.go:171] initial CRD sync complete...
I0202 08:10:47.415750       1 autoregister_controller.go:144] Starting autoregister controller
I0202 08:10:47.415756       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0202 08:10:47.413707       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0202 08:10:47.513413       1 shared_informer.go:320] Caches are synced for configmaps
I0202 08:10:47.613948       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0202 08:10:47.614058       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0202 08:10:47.614167       1 policy_source.go:224] refreshing policies
I0202 08:10:47.616611       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0202 08:10:47.616712       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0202 08:10:47.616880       1 cache.go:39] Caches are synced for autoregister controller
I0202 08:10:47.618975       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0202 08:10:47.628335       1 cache.go:39] Caches are synced for LocalAvailability controller
I0202 08:10:47.628366       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0202 08:10:47.628746       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0202 08:10:47.727706       1 shared_informer.go:320] Caches are synced for node_authorizer
I0202 08:10:47.733622       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
E0202 08:10:47.913669       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0202 08:10:48.421051       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0202 08:10:51.533739       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0202 08:10:51.615722       1 controller.go:615] quota admission added evaluator for: endpoints
I0202 10:09:11.058913       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0202 10:09:11.959534       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]


==> kube-apiserver [1fe5d0577dda] <==
W0223 11:03:13.891985       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0223 11:03:15.365031       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0223 11:03:15.366187       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0223 11:03:15.367420       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0223 11:03:15.372131       1 secure_serving.go:213] Serving securely on [::]:8443
I0223 11:03:15.372510       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0223 11:03:15.372820       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0223 11:03:15.372970       1 aggregator.go:169] waiting for initial CRD sync...
I0223 11:03:15.373045       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0223 11:03:15.373131       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0223 11:03:15.373592       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0223 11:03:15.377044       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0223 11:03:15.377072       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0223 11:03:15.377178       1 controller.go:78] Starting OpenAPI AggregationController
I0223 11:03:15.380902       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0223 11:03:15.382577       1 local_available_controller.go:156] Starting LocalAvailability controller
I0223 11:03:15.382608       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0223 11:03:15.382687       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0223 11:03:15.382701       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0223 11:03:15.383185       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0223 11:03:15.383264       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0223 11:03:15.385080       1 controller.go:90] Starting OpenAPI V3 controller
I0223 11:03:15.385156       1 controller.go:142] Starting OpenAPI controller
I0223 11:03:15.385723       1 naming_controller.go:294] Starting NamingConditionController
I0223 11:03:15.386089       1 establishing_controller.go:81] Starting EstablishingController
I0223 11:03:15.386255       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0223 11:03:15.386281       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0223 11:03:15.386302       1 crd_finalizer.go:269] Starting CRDFinalizer
I0223 11:03:15.389749       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0223 11:03:15.390279       1 controller.go:119] Starting legacy_token_tracking_controller
I0223 11:03:15.390295       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0223 11:03:15.390397       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0223 11:03:15.405489       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I0223 11:03:15.405554       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0223 11:03:15.439713       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0223 11:03:15.657336       1 shared_informer.go:320] Caches are synced for configmaps
I0223 11:03:15.658465       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0223 11:03:15.658486       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0223 11:03:15.667066       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0223 11:03:15.667201       1 policy_source.go:224] refreshing policies
I0223 11:03:15.739743       1 shared_informer.go:320] Caches are synced for node_authorizer
I0223 11:03:15.739857       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0223 11:03:15.739909       1 cache.go:39] Caches are synced for LocalAvailability controller
I0223 11:03:15.740520       1 aggregator.go:171] initial CRD sync complete...
I0223 11:03:15.740561       1 autoregister_controller.go:144] Starting autoregister controller
I0223 11:03:15.740578       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0223 11:03:15.740594       1 cache.go:39] Caches are synced for autoregister controller
I0223 11:03:15.745679       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0223 11:03:15.745750       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0223 11:03:15.749106       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0223 11:03:15.758172       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0223 11:03:15.849678       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
E0223 11:03:15.875910       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
E0223 11:03:15.955038       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: cd8d09c9-f317-4559-840e-00188e0d4e75, UID in object meta: "
I0223 11:03:16.657028       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0223 11:03:27.146615       1 controller.go:615] quota admission added evaluator for: endpoints
I0223 11:03:27.966990       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0223 11:29:54.956741       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0223 11:29:55.052013       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0223 11:30:06.991925       1 alloc.go:330] "allocated clusterIPs" service="default/ibmapp-app-service" clusterIPs={"IPv4":"10.111.112.150"}


==> kube-controller-manager [2c394814175e] <==
I0202 09:56:16.585315       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="77.011µs"
I0202 09:56:21.593234       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="73.211µs"
I0202 09:56:29.604090       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="63.51µs"
I0202 09:56:33.613415       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="70.31µs"
I0202 09:56:46.045374       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="248.17158ms"
I0202 09:56:46.050148       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="86.612µs"
I0202 09:56:46.882548       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="13.924051ms"
I0202 09:56:46.882755       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="73.511µs"
I0202 09:56:49.599582       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/cuboid-774b475b8c" duration="101.516µs"
I0202 09:57:00.583319       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="76.611µs"
I0202 09:57:02.601449       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/cuboid-774b475b8c" duration="63.309µs"
I0202 09:57:31.579187       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="69.81µs"
I0202 09:57:45.606908       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="90.213µs"
I0202 09:58:31.360140       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0202 10:01:29.555105       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="71.91µs"
I0202 10:01:31.561300       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="73.01µs"
I0202 10:01:42.584913       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="139.117µs"
I0202 10:01:42.618127       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="69.309µs"
I0202 10:01:52.587066       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/cuboid-774b475b8c" duration="158.422µs"
I0202 10:01:54.817612       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="79.987664ms"
I0202 10:01:54.822602       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="124.217µs"
I0202 10:01:56.231183       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="81.750208ms"
I0202 10:01:56.231387       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="80.311µs"
I0202 10:02:06.576633       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/cuboid-774b475b8c" duration="168.823µs"
I0202 10:02:07.586497       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="123.817µs"
I0202 10:02:41.568116       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="102.514µs"
I0202 10:02:54.552965       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="141.919µs"
I0202 10:03:36.964130       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0202 10:06:35.589469       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="597.478µs"
I0202 10:06:46.532922       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="112.115µs"
I0202 10:06:47.583505       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="77.31µs"
I0202 10:06:59.533823       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="117.417µs"
I0202 10:07:03.677389       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/cuboid-774b475b8c" duration="276.638µs"
I0202 10:07:07.069774       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="94.793885ms"
I0202 10:07:07.074139       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="79.01µs"
I0202 10:07:09.175030       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="83.488684ms"
I0202 10:07:09.179462       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="93.412µs"
I0202 10:07:16.581539       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/cuboid-774b475b8c" duration="60.807µs"
I0202 10:07:20.524824       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="86.612µs"
I0202 10:07:49.529197       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="128.718µs"
I0202 10:08:04.501639       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="126.118µs"
I0202 10:08:43.360995       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0202 10:08:56.940597       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-deployment-57dbf8c5d4" duration="13.301µs"
I0202 10:08:56.940981       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-8594c84c4b" duration="7.201µs"
I0202 10:08:56.940361       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-deployment-5b6fb97f79" duration="22.303µs"
I0202 10:08:56.947333       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-deployment-67dfbf9bdf" duration="9.801µs"
I0202 10:08:56.945742       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/devops-deployment-59b78d8d49" duration="6.501µs"
I0202 10:08:56.946508       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-deployment-d884b6df9" duration="546.276µs"
I0202 10:08:56.946894       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/cuboid-774b475b8c" duration="8.901µs"
I0202 10:08:56.945710       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="9.102µs"
I0202 10:08:56.967578       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-568cc95c44" duration="5.401µs"
I0202 10:08:57.047958       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/spook-569b7df956" duration="9.902µs"
I0202 10:08:57.743699       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/ibmapp-service" err="EndpointSlice informer cache is out of date"
I0202 10:08:58.155242       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/test-85f9945c9" duration="5.601µs"
I0202 10:13:50.207365       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0202 10:18:56.848942       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0202 10:24:04.424120       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0202 10:29:10.595433       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0202 10:34:16.824606       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0202 10:39:23.379709       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [35aaf06125db] <==
I0223 11:03:26.457768       1 shared_informer.go:320] Caches are synced for taint
I0223 11:03:26.458008       1 shared_informer.go:320] Caches are synced for persistent volume
I0223 11:03:26.458015       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0223 11:03:26.458349       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0223 11:03:26.461050       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0223 11:03:26.461680       1 shared_informer.go:320] Caches are synced for job
I0223 11:03:26.458484       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0223 11:03:26.857324       1 shared_informer.go:320] Caches are synced for garbage collector
I0223 11:03:26.944242       1 shared_informer.go:320] Caches are synced for garbage collector
I0223 11:03:26.944344       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0223 11:03:27.941337       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="1.573818643s"
I0223 11:03:27.944215       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="240.893µs"
I0223 11:03:28.144798       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="1.700580054s"
I0223 11:03:28.145264       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="kube-system/kube-dns" err="EndpointSlice informer cache is out of date"
I0223 11:03:28.145467       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="1.701068939s"
I0223 11:03:28.146425       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="79.698µs"
I0223 11:03:28.148692       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="70.398µs"
I0223 11:03:28.243106       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="kubernetes-dashboard/dashboard-metrics-scraper" err="EndpointSlice informer cache is out of date"
I0223 11:03:28.243248       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="kubernetes-dashboard/kubernetes-dashboard" err="EndpointSlice informer cache is out of date"
I0223 11:03:41.231676       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="154.749898ms"
I0223 11:03:41.248178       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="55.539µs"
I0223 11:03:41.377197       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="115.392226ms"
I0223 11:03:41.377314       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="71.851µs"
I0223 11:03:41.399393       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="112.28µs"
I0223 11:04:02.976217       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="37.123414ms"
I0223 11:04:02.976463       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="85.151µs"
I0223 11:04:03.965483       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="72.643µs"
I0223 11:04:13.317096       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="40.101429ms"
I0223 11:04:13.317477       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="136.13µs"
I0223 11:04:19.528259       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="25.610579ms"
I0223 11:04:19.528726       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="182.975µs"
I0223 11:08:22.935868       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0223 11:13:29.250669       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0223 11:18:35.059582       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0223 11:23:41.874508       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0223 11:28:47.509608       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0223 11:29:55.195004       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="123.998259ms"
I0223 11:29:55.235150       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="39.973316ms"
I0223 11:29:55.239206       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="1.342268ms"
I0223 11:29:55.370477       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="2.562322ms"
I0223 11:29:55.448241       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="765.897µs"
I0223 11:30:05.985887       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="84.811µs"
I0223 11:30:17.417914       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="77.612µs"
I0223 11:30:36.627740       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="248.031µs"
I0223 11:30:49.513977       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="95.813µs"
I0223 11:31:06.508958       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="139.816µs"
I0223 11:31:20.431588       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="60.809µs"
I0223 11:32:05.411029       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="157.721µs"
I0223 11:32:18.482467       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="143.419µs"
I0223 11:33:32.465188       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="402.613µs"
I0223 11:33:44.418481       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="224.428µs"
I0223 11:33:54.450980       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0223 11:36:26.411713       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="130.416µs"
I0223 11:36:37.403061       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="63.508µs"
I0223 11:39:00.780296       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0223 11:41:35.347877       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="421.623µs"
I0223 11:41:46.326907       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="60.007µs"
I0223 11:44:07.395105       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0223 11:46:43.281837       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="132.626µs"
I0223 11:46:58.282739       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ibmapp-67dfbf9bdf" duration="5.175354ms"


==> kube-proxy [0b3bb8302555] <==
E0202 08:10:57.564159       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0202 08:10:57.573628       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0202 08:10:57.631985       1 server_linux.go:66] "Using iptables proxy"
I0202 08:10:58.016873       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0202 08:10:58.017262       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0202 08:10:58.067489       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0202 08:10:58.067617       1 server_linux.go:169] "Using iptables Proxier"
I0202 08:10:58.070850       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0202 08:10:58.080357       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0202 08:10:58.089778       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0202 08:10:58.089941       1 server.go:483] "Version info" version="v1.31.0"
I0202 08:10:58.089996       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0202 08:10:58.093671       1 config.go:104] "Starting endpoint slice config controller"
I0202 08:10:58.093747       1 config.go:197] "Starting service config controller"
I0202 08:10:58.094043       1 config.go:326] "Starting node config controller"
I0202 08:10:58.094688       1 shared_informer.go:313] Waiting for caches to sync for service config
I0202 08:10:58.094701       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0202 08:10:58.095528       1 shared_informer.go:313] Waiting for caches to sync for node config
I0202 08:10:58.195275       1 shared_informer.go:320] Caches are synced for service config
I0202 08:10:58.195275       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0202 08:10:58.195884       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [332ca9e61afe] <==
E0223 11:03:40.926815       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0223 11:03:40.968595       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0223 11:03:41.156898       1 server_linux.go:66] "Using iptables proxy"
I0223 11:03:42.068580       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0223 11:03:42.069244       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0223 11:03:42.208633       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0223 11:03:42.209225       1 server_linux.go:169] "Using iptables Proxier"
I0223 11:03:42.225942       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0223 11:03:42.248320       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0223 11:03:42.279198       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0223 11:03:42.279662       1 server.go:483] "Version info" version="v1.31.0"
I0223 11:03:42.279761       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0223 11:03:42.297739       1 config.go:197] "Starting service config controller"
I0223 11:03:42.297810       1 config.go:326] "Starting node config controller"
I0223 11:03:42.297926       1 shared_informer.go:313] Waiting for caches to sync for service config
I0223 11:03:42.297739       1 config.go:104] "Starting endpoint slice config controller"
I0223 11:03:42.298058       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0223 11:03:42.298215       1 shared_informer.go:313] Waiting for caches to sync for node config
I0223 11:03:42.398242       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0223 11:03:42.398299       1 shared_informer.go:320] Caches are synced for service config
I0223 11:03:42.398413       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [b6976d8afe11] <==
I0202 08:10:44.538285       1 serving.go:386] Generated self-signed cert in-memory
I0202 08:10:47.923724       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0202 08:10:47.923772       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0202 08:10:48.018392       1 requestheader_controller.go:172] Starting RequestHeaderAuthRequestController
I0202 08:10:48.018414       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0202 08:10:48.018626       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0202 08:10:48.018873       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0202 08:10:48.019045       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0202 08:10:48.019111       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I0202 08:10:48.019129       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0202 08:10:48.019149       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0202 08:10:48.122944       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I0202 08:10:48.124463       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0202 08:10:48.124679       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file


==> kube-scheduler [cb910300419c] <==
I0223 11:03:07.462539       1 serving.go:386] Generated self-signed cert in-memory
W0223 11:03:15.551106       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0223 11:03:15.551265       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0223 11:03:15.551329       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W0223 11:03:15.551385       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0223 11:03:16.156795       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I0223 11:03:16.156851       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0223 11:03:16.264067       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0223 11:03:16.264903       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0223 11:03:16.265574       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0223 11:03:16.340576       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0223 11:03:16.567396       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Feb 23 11:35:56 minikube kubelet[1649]: E0223 11:35:56.363601    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:36:14 minikube kubelet[1649]: E0223 11:36:14.282525    1649 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rohittrathod/ibmproject:latest"
Feb 23 11:36:14 minikube kubelet[1649]: E0223 11:36:14.282662    1649 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rohittrathod/ibmproject:latest"
Feb 23 11:36:14 minikube kubelet[1649]: E0223 11:36:14.282938    1649 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:ibmapp-container,Image:rohittrathod/ibmproject:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vhbwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibmapp-67dfbf9bdf-gkhfw_default(db5ef8e7-b90f-4318-947d-ec3f753fe03f): ErrImagePull: Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Feb 23 11:36:14 minikube kubelet[1649]: E0223 11:36:14.284409    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ErrImagePull: \"Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:36:26 minikube kubelet[1649]: E0223 11:36:26.341694    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:36:37 minikube kubelet[1649]: E0223 11:36:37.350916    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:36:52 minikube kubelet[1649]: E0223 11:36:52.340871    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:37:06 minikube kubelet[1649]: E0223 11:37:06.342367    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:37:20 minikube kubelet[1649]: E0223 11:37:20.339603    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:37:34 minikube kubelet[1649]: E0223 11:37:34.336999    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:37:45 minikube kubelet[1649]: E0223 11:37:45.335110    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:37:56 minikube kubelet[1649]: E0223 11:37:56.335682    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:38:08 minikube kubelet[1649]: E0223 11:38:08.328672    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:38:19 minikube kubelet[1649]: E0223 11:38:19.325982    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:38:34 minikube kubelet[1649]: E0223 11:38:34.328995    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:38:49 minikube kubelet[1649]: E0223 11:38:49.325002    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:39:00 minikube kubelet[1649]: E0223 11:39:00.325876    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:39:15 minikube kubelet[1649]: E0223 11:39:15.317546    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:39:30 minikube kubelet[1649]: E0223 11:39:30.318714    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:39:44 minikube kubelet[1649]: E0223 11:39:44.322767    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:39:55 minikube kubelet[1649]: E0223 11:39:55.314327    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:40:09 minikube kubelet[1649]: E0223 11:40:09.314308    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:40:22 minikube kubelet[1649]: E0223 11:40:22.315487    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:40:34 minikube kubelet[1649]: E0223 11:40:34.333260    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:40:48 minikube kubelet[1649]: E0223 11:40:48.308399    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:41:02 minikube kubelet[1649]: E0223 11:41:02.317320    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:41:23 minikube kubelet[1649]: E0223 11:41:23.209784    1649 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rohittrathod/ibmproject:latest"
Feb 23 11:41:23 minikube kubelet[1649]: E0223 11:41:23.211076    1649 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rohittrathod/ibmproject:latest"
Feb 23 11:41:23 minikube kubelet[1649]: E0223 11:41:23.211350    1649 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:ibmapp-container,Image:rohittrathod/ibmproject:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vhbwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibmapp-67dfbf9bdf-gkhfw_default(db5ef8e7-b90f-4318-947d-ec3f753fe03f): ErrImagePull: Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Feb 23 11:41:23 minikube kubelet[1649]: E0223 11:41:23.212906    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ErrImagePull: \"Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:41:35 minikube kubelet[1649]: E0223 11:41:35.302681    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:41:46 minikube kubelet[1649]: E0223 11:41:46.300366    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:41:57 minikube kubelet[1649]: E0223 11:41:57.303730    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:42:11 minikube kubelet[1649]: E0223 11:42:11.297427    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:42:26 minikube kubelet[1649]: E0223 11:42:26.305179    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:42:39 minikube kubelet[1649]: E0223 11:42:39.295885    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:42:51 minikube kubelet[1649]: E0223 11:42:51.289702    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:43:03 minikube kubelet[1649]: E0223 11:43:03.295572    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:43:18 minikube kubelet[1649]: E0223 11:43:18.292934    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:43:32 minikube kubelet[1649]: E0223 11:43:32.290312    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:43:45 minikube kubelet[1649]: E0223 11:43:45.278125    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:43:58 minikube kubelet[1649]: E0223 11:43:58.283934    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:44:12 minikube kubelet[1649]: E0223 11:44:12.271233    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:44:23 minikube kubelet[1649]: E0223 11:44:23.275788    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:44:36 minikube kubelet[1649]: E0223 11:44:36.274346    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:44:47 minikube kubelet[1649]: E0223 11:44:47.271832    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:45:01 minikube kubelet[1649]: E0223 11:45:01.283215    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:45:16 minikube kubelet[1649]: E0223 11:45:16.260264    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:45:31 minikube kubelet[1649]: E0223 11:45:31.266831    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:45:45 minikube kubelet[1649]: E0223 11:45:45.255682    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:45:58 minikube kubelet[1649]: E0223 11:45:58.255215    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:46:14 minikube kubelet[1649]: E0223 11:46:14.249428    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:46:30 minikube kubelet[1649]: E0223 11:46:30.017895    1649 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rohittrathod/ibmproject:latest"
Feb 23 11:46:30 minikube kubelet[1649]: E0223 11:46:30.018195    1649 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rohittrathod/ibmproject:latest"
Feb 23 11:46:30 minikube kubelet[1649]: E0223 11:46:30.018628    1649 kuberuntime_manager.go:1272] "Unhandled Error" err="container &Container{Name:ibmapp-container,Image:rohittrathod/ibmproject:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vhbwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod ibmapp-67dfbf9bdf-gkhfw_default(db5ef8e7-b90f-4318-947d-ec3f753fe03f): ErrImagePull: Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Feb 23 11:46:30 minikube kubelet[1649]: E0223 11:46:30.023379    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ErrImagePull: \"Error response from daemon: pull access denied for rohittrathod/ibmproject, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:46:43 minikube kubelet[1649]: E0223 11:46:43.244601    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:46:58 minikube kubelet[1649]: E0223 11:46:58.267121    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"
Feb 23 11:47:13 minikube kubelet[1649]: E0223 11:47:13.237471    1649 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ibmapp-container\" with ImagePullBackOff: \"Back-off pulling image \\\"rohittrathod/ibmproject:latest\\\"\"" pod="default/ibmapp-67dfbf9bdf-gkhfw" podUID="db5ef8e7-b90f-4318-947d-ec3f753fe03f"


==> kubernetes-dashboard [261dfd111058] <==
2025/02/23 11:03:40 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: connect: connection refused

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00055fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc000430180)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2025/02/23 11:03:40 Using namespace: kubernetes-dashboard
2025/02/23 11:03:40 Using in-cluster config to connect to apiserver
2025/02/23 11:03:40 Using secret token for csrf signing
2025/02/23 11:03:40 Initializing csrf token from kubernetes-dashboard-csrf secret


==> kubernetes-dashboard [99ba2164972a] <==
2025/02/23 11:04:18 Starting overwatch
2025/02/23 11:04:18 Using namespace: kubernetes-dashboard
2025/02/23 11:04:18 Using in-cluster config to connect to apiserver
2025/02/23 11:04:18 Using secret token for csrf signing
2025/02/23 11:04:18 Initializing csrf token from kubernetes-dashboard-csrf secret
2025/02/23 11:04:18 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2025/02/23 11:04:18 Successful initial request to the apiserver, version: v1.31.0
2025/02/23 11:04:18 Generating JWE encryption key
2025/02/23 11:04:18 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2025/02/23 11:04:18 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2025/02/23 11:04:18 Initializing JWE encryption key from synchronized object
2025/02/23 11:04:18 Creating in-cluster Sidecar client
2025/02/23 11:04:18 Successful request to sidecar
2025/02/23 11:04:18 Serving insecurely on HTTP port: 9090


==> storage-provisioner [ca761964fa84] <==
I0223 11:04:13.221426       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0223 11:04:13.264981       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0223 11:04:13.266645       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0223 11:04:30.719682       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0223 11:04:30.720175       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_29f74246-9aa5-45ff-a353-849cf34b4137!
I0223 11:04:30.720130       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"173c6734-3131-4711-b4c8-72764e448737", APIVersion:"v1", ResourceVersion:"53479", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_29f74246-9aa5-45ff-a353-849cf34b4137 became leader
I0223 11:04:30.822088       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_29f74246-9aa5-45ff-a353-849cf34b4137!


==> storage-provisioner [e7db91fd2832] <==
I0223 11:03:37.227853       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0223 11:03:58.329888       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

